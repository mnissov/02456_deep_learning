{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import os\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('DR_Data/rentals.pickle')\n",
    "\n",
    "\n",
    "X=np.array(data.pickup_zone)\n",
    "y=np.array(data.dropoff_zone)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "num_zones = max(X_train)+1\n",
    "X_train_enc=np.zeros((X_train.shape[0],num_zones))\n",
    "X_valid_enc=np.zeros((X_valid.shape[0],num_zones))\n",
    "y_train_enc=np.zeros((X_train.shape[0],num_zones))\n",
    "y_valid_enc=np.zeros((X_valid.shape[0],num_zones))\n",
    "for i in range(len(X_train)):\n",
    "    X_train_enc[i][X_train[i]]=1\n",
    "    y_train_enc[i][y_train[i]]=1\n",
    "for i in range(len(X_valid)):\n",
    "    X_valid_enc[i][X_valid[i]]=1\n",
    "    y_valid_enc[i][y_valid[i]]=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GPU.\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    #x=x.type(torch.FloatTensor)\n",
    "    if use_cuda:\n",
    "        x = x.cuda()\n",
    "        return x\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BUILDING MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_classes = num_zones\n",
    "num_l1 = 256\n",
    "num_l2 = 128\n",
    "num_l3 = 128\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0\n",
    "\n",
    "num_features = num_zones\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden1, num_hidden2, num_output):\n",
    "        super(Net, self).__init__()\n",
    "        # input layer\n",
    "        self.W_1 = Parameter(init.kaiming_normal_(torch.Tensor(num_hidden1, num_features)))\n",
    "        self.b_1 = Parameter(init.constant_(torch.Tensor(num_hidden1), 0))\n",
    "        # hidden layer 1\n",
    "        self.W_2 = Parameter(init.kaiming_normal_(torch.Tensor(num_hidden2, num_hidden1)))\n",
    "        self.b_2 = Parameter(init.constant_(torch.Tensor(num_hidden2), 0))\n",
    "        # hidden layer 2\n",
    "        self.W_3 = Parameter(init.kaiming_normal_(torch.Tensor(num_output, num_hidden2)))\n",
    "        self.b_3 = Parameter(init.constant_(torch.Tensor(num_output), 0))\n",
    "        # define activation function in constructor\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        # define dropout to reduce overfitting\n",
    "        self.drop = torch.nn.Dropout(p=0.5)\n",
    "        # define batchnorm\n",
    "        self.batch1 = torch.nn.BatchNorm1d(num_hidden1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # forward prop\n",
    "        #x=x.type(torch.FloatTensor)\n",
    "        x = F.linear(x, self.W_1, self.b_1)\n",
    "        x = self.activation(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.batch1(x)\n",
    "        x = F.linear(x, self.W_2, self.b_2)\n",
    "        x = self.activation(x)\n",
    "        x = F.linear(x, self.W_3, self.b_3)\n",
    "        # dropout\n",
    "        \n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "net = Net(num_features, num_l1, num_l2, num_classes)\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "    \n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-923308bdf996>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mt_start2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mx_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_enc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslce\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mt_end2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mt_avg2\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mt_end1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt_start1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-c4358490ce5b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# forward prop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m#x=x.type(torch.FloatTensor)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1368\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# setting hyperparameters and gettings epoch sizes\n",
    "batch_size = 512\n",
    "num_epochs = 100\n",
    "num_samples_train = X_train_enc.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = X_valid_enc.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "losses = []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    ## Train\n",
    "    \n",
    "    t_avg1 = 0;\n",
    "    t_avg2 = 0;\n",
    "    t_avg3 = 0;\n",
    "    t_avg4 = 0;\n",
    "\n",
    "    t_start_1 = time.time()\n",
    "    cur_loss = 0\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    t_start4 = time.time()\n",
    "    for i in range(num_batches_train):\n",
    "        \n",
    "        t_start1 = time.time()\n",
    "        slce = get_slice(i, batch_size)\n",
    "        t_end1 = time.time()\n",
    "        t_avg1 += t_end1-t_start1\n",
    "        \n",
    "        t_start2 = time.time()\n",
    "        x_batch = get_variable(Variable(torch.from_numpy(X_train_enc[slce])).type(torch.FloatTensor))\n",
    "        output = net(x_batch)\n",
    "        t_end2 = time.time()\n",
    "        t_avg2 += t_end1-t_start1\n",
    "        \n",
    "        # compute gradients given loss\n",
    "        t_start3 = time.time()\n",
    "        target_batch = get_variable(Variable(torch.from_numpy(y_train[slce]).long()))\n",
    "        batch_loss = criterion(output, target_batch)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        t_end3 = time.time()\n",
    "        t_avg3 += t_end1-t_start1\n",
    "        \n",
    "        cur_loss += batch_loss \n",
    "        \n",
    "        t_end4 = time.time()\n",
    "        t_avg4 += t_end4-t_start4\n",
    "        t_start4 = time.time()\n",
    "        \n",
    "    losses.append(cur_loss / batch_size)\n",
    "    \n",
    "    t_end_1 = time.time()\n",
    "    \n",
    "    #print(\"Get slice\", t_avg1)\n",
    "    #print(\"Get vble + fwd pass\", t_avg2)\n",
    "    #print(\"Learning stuff\", t_avg3)\n",
    "    #print(\"Loop time\", t_avg4)\n",
    "    print(\"All training\", t_end_1 - t_start_1)\n",
    "    \n",
    "    net.eval()\n",
    "    ### Evaluate training\n",
    "    train_preds, train_targs = [], []\n",
    "    for i in range(num_batches_train):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        x_batch = get_variable(Variable(torch.from_numpy(X_train_enc[slce])).type(torch.FloatTensor))\n",
    "        \n",
    "        output = net(x_batch)\n",
    "        preds = torch.max(output, 1)[1]\n",
    "        \n",
    "        train_targs += list(y_train[slce])\n",
    "        train_preds += list(get_numpy(preds))\n",
    "    \n",
    "    ### Evaluate validation\n",
    "    val_preds, val_targs = [], []\n",
    "    for i in range(num_batches_valid):\n",
    "        slce = get_slice(i, batch_size)\n",
    "        x_batch = get_variable(Variable(torch.from_numpy(X_valid_enc[slce])).type(torch.FloatTensor))\n",
    "        \n",
    "        output = net(x_batch)\n",
    "        preds = torch.max(output, 1)[1]\n",
    "        val_preds += list(get_numpy(preds))\n",
    "        val_targs += list(y_valid[slce])\n",
    "\n",
    "    train_acc_cur = accuracy_score(train_targs, train_preds)\n",
    "    valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
    "    \n",
    "    train_acc.append(train_acc_cur)\n",
    "    valid_acc.append(valid_acc_cur)\n",
    "    \n",
    "    if epoch % 4 == 0:\n",
    "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
    "                epoch+1, losses[-1], train_acc_cur, valid_acc_cur))\n",
    "\n",
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(get_variable(Variable(torch.from_numpy(X_valid_enc))).type(torch.FloatTensor))\n",
    "plt.plot(output[10].detach().numpy())\n",
    "print(np.argmax(X_valid_enc[0]))\n",
    "print(y_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_batches_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
