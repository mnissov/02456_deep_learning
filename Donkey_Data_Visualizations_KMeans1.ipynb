{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams[\"figure.figsize\"] = (16,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating distance (km) between two cordinates\n",
    "def distance(lat1, long1, lat2, long2):\n",
    "    #Radius in km\n",
    "    R = 6371\n",
    "\n",
    "    lat1=np.deg2rad(lat1)\n",
    "    lat2=np.deg2rad(lat2)\n",
    "    long1=np.deg2rad(long1)\n",
    "    long2=np.deg2rad(long2)\n",
    "    dlong = long2 - long1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.power((np.sin(dlat/2)), 2) + np.cos(lat1) * np.cos(lat2) * np.power(np.sin(dlong/2), 2)\n",
    "    c = 2 * np.arctan2( np.sqrt(a), np.sqrt(1-a) )\n",
    "    d = R * c #(where R is the radius of the Earth)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------\n",
    "\n",
    "# Data preparation and description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by simply importing the datasets and doing some initial restructuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Data/Rentals_2019-4-2_1456.csv' does not exist: b'Data/Rentals_2019-4-2_1456.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ad2693071935>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# importing data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_rentals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data/Rentals_2019-4-2_1456.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_hubav\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data/Hubavailabilityaudits_2019-4-2_1535.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_hubs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data/Hubs_2019-4-2_1201.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_searchlog1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data/Searchlogs_2019-4-2_1234.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Data/Rentals_2019-4-2_1456.csv' does not exist: b'Data/Rentals_2019-4-2_1456.csv'"
     ]
    }
   ],
   "source": [
    "# importing data\n",
    "df_rentals = pd.read_csv(\"Data/Rentals_2019-4-2_1456.csv\", parse_dates=[0,1])\n",
    "df_hubav = pd.read_csv(\"Data/Hubavailabilityaudits_2019-4-2_1535.csv\", parse_dates=[3])\n",
    "df_hubs = pd.read_csv(\"Data/Hubs_2019-4-2_1201.csv\", parse_dates=[0,-1])\n",
    "df_searchlog1 = pd.read_csv(\"Data/Searchlogs_2019-4-2_1234.csv\", parse_dates=[-1])\n",
    "df_searchlog2 = pd.read_csv(\"Data/Searchlogs_2019-4-2_1504.csv\", parse_dates=[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenating searchlogs dataframes (the second file is a continuation of the first)\n",
    "df_searchlog = pd.concat([df_searchlog1, df_searchlog2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will be dealing with belongs to Donkey Republic, a bike-sharing company operating in Copenhagen. The service offered by Donkey Republic falls under the family of *hub-based* bike-sharing services. This means that bikes can only be picked-up and dropped-off in specific areas - i.e. \"virtual\" areas (meaning that no physical platforms are installed) called hubs. Our goal will be to define a model for the prediction of bike usage demand in the various hubs. Having accurate predictions of demand will be the key element in order to take coherent decisions on the supply side (supply dimensioning, rebalancing, hub positioning, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset will be constituted by various measurements of the bike usage and of the service provider. The information available to us is organized in the following way:\n",
    "\n",
    "1) *df_rentals*: represents the actual bike rentals indicating time and hub for both pick-up and drop-off, together with the user id associated with that rental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rentals.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) *df_hubs*: describes various characteristics for each hub such as time of creation, geo-spatial location, id, name and (eventual) time of deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hubs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) *df_searchlog*: represents the geo-localization of users in the moment they access the app. For every search it describes location, time and user_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_searchlog.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) *df_hubav*: represents the availability of bikes for every hub and how it evolved with time. In particular, at a certain timestamp \"created_at\", the bike count in a specific hub goes from the value in \"bike_count_from\" to the one in \"bike_count_to\". To give an example, if at a certain timestamp there has been a drop-off, and the previous availability of bikes was 8 (bike_count_from), then the new availability would be 9 (bike_count_to). \n",
    "\n",
    "This table also shows values for optimal and maximum capacity of bikes for every hub. It is already possible to notice some anamalous values in the data (i.e. 0 maximum capacity) which definitely need some further inspection..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hubav.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going in more datail, here are some first general descriptions for the our four tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rentals.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hubs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_searchlog.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hubav.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hubs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows we aim at visualizing on the map of Copenhagen all the existing (and deleted) hubs in our dataset. In order to do this we first need to differentiate between the still existing-hubs and the deleted ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centered location (latitude, longitude) which we will use in all our following plots\n",
    "initial_location = [55.6775757, 12.579571639999999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deleted_hubs = df_hubs.where(pd.notna(df_hubs.deleted_at) == True).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deleted_hubs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_existing_hubs = df_hubs.where(pd.notna(df_hubs.deleted_at) == False).dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_existing_hubs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we are ready to plot the hubs on the map differentiating the existing (in blue) from the deleted (in red) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting latitude-longitude locations for all the hubs\n",
    "existing_locations = df_existing_hubs.iloc[:, 1:3].values.tolist()\n",
    "del_hub_locations = df_deleted_hubs.iloc[:, 1:3].values.tolist()\n",
    "\n",
    "np.array(existing_locations).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_superhubs = 100\n",
    "margin = 0.01\n",
    "\n",
    "# Preprocessing (flip and rotate 90deg clockwise (mirror in y=x, change x by y))\n",
    "existing_locations_arr = np.array(existing_locations)\n",
    "existing_locations_arr[:,[0,1]] = existing_locations_arr[:,[1,0]]\n",
    "\n",
    "\n",
    "kmeans = KMeans(init='k-means++', n_clusters=n_superhubs, n_init=10)\n",
    "kmeans.fit(existing_locations_arr)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .0005     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = existing_locations_arr[:, 0].min() - margin, existing_locations_arr[:, 0].max() + margin\n",
    "y_min, y_max = existing_locations_arr[:, 1].min() - margin, existing_locations_arr[:, 1].max() + margin\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "print(Z.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.terrain,\n",
    "           aspect='equal', origin='lower')\n",
    "\n",
    "plt.plot(existing_locations_arr[:, 0], existing_locations_arr[:, 1], 'k.', markersize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hub_map = folium.Map(location=initial_location, zoom_start=13)\n",
    "\n",
    "for point in range(0, len(existing_locations)):\n",
    "    folium.CircleMarker(\n",
    "    location=existing_locations[point],\n",
    "    radius=3,\n",
    "    popup='Hub_{}'.format(df_existing_hubs.iloc[point, 3]),\n",
    "    color='#363636',\n",
    "    fill=True,\n",
    "    fill_color='#363636').add_to(hub_map)\n",
    "    \n",
    "'''for point in range(0, len(del_hub_locations)):\n",
    "    folium.CircleMarker(\n",
    "    location=del_hub_locations[point],\n",
    "    radius=3,\n",
    "    popup='Hub_{}'.format(df_deleted_hubs.iloc[point, 3]),\n",
    "    color='crimson',\n",
    "    fill=True,\n",
    "    fill_color='crimson').add_to(hub_map)'''\n",
    "    \n",
    "folium.raster_layers.ImageOverlay(Z,\n",
    "                                  bounds=[[yy.min(),xx.min()],[yy.max(),xx.max()]],\n",
    "                                  opacity=0.5,\n",
    "                                  colormap=plt.cm.prism,\n",
    "                                  origin='lower').add_to(hub_map)\n",
    "\n",
    "hub_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being our goal the prediction of bike demand in the various hubs, the \"Rentals\" DataFrame definitely plays a central role. This will be the source of our observed demand in each hub. As we have seen above, this dataset is for now missing any kind of aggregation, meaning that our bike rentals are only recorded as individual instances on the timeline of our data.\n",
    "\n",
    "This definitely leaves a lot of space for modeling: we could choose to aggregate our data at different levels (e.g. we could count the rentals for every hour/day/week/month). Obviously this depends on what is the type of decisions we want to take after having extracted information from our data. In our case, the decision will mostly have (at least initially) an operational level i.e. short-medium term. Therefore we are naturally more interested in a higher granularity of our data. \n",
    "\n",
    "For example, the following plots regard the daily time-series of pick-ups in a specific hub (Hub #60).\n",
    "\n",
    "(before actually being able to plot our data we must fill in a \"0\" value for all those days where we don't have any recorded rental, in what follows we construct a DataFrame df_hub_60 which has the complete time series of pick-ups in Hub #60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_index = pd.DatetimeIndex(pd.DatetimeIndex(df_rentals[df_rentals.pickup_hub_id == 60].created_at).date)\n",
    "complete_index = pd.date_range(incomplete_index.min(),incomplete_index.max())\n",
    "complete_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two following cells give a representation of the type of imputation we just implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_rentals[df_rentals.pickup_hub_id == 60].created_at.dt.floor('d') \\\n",
    ".value_counts().sort_index().fillna(value=0).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_rentals[df_rentals.pickup_hub_id == 60].created_at.dt.floor('d').value_counts().sort_index() \\\n",
    ".reindex(complete_index).fillna(value=0).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to plot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rentals[df_rentals.pickup_hub_id == 60].created_at.dt.floor('d').value_counts().sort_index(). \\\n",
    "reindex(complete_index).fillna(value=0).plot(style='-')\n",
    "plt.title(\"Pick-up Request in Hub #60 (Norreport Area)\")\n",
    "plt.ylabel(\"Number of Pick-ups\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot we can observe a relevant increment in the observed demand especially during the summer (or close-to-summer) months. Other than leading us to explore some sort of seasonality this could also suggest that our data is actually generated from two distinct processes...\n",
    "\n",
    "In the following graph we try to give a sense of this dual nature by plotting the histogram of the pickup counts (in Hub #60) by roughly trying to isolate the central part of our time-series: the \"*nice weather*\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a rough separation the data is devided in 3 parts (using the \"official\" summer dates: 21 Jun - 23 Sep)\n",
    "summer_start_idx = int(np.where(df_rentals[df_rentals.pickup_hub_id == 60].created_at.dt.floor('d').value_counts()\n",
    "                    .sort_index().index == pd.to_datetime('2018-06-21'))[0])\n",
    "summer_end_idx = int(np.where(df_rentals[df_rentals.pickup_hub_id == 60].created_at.dt.floor('d').value_counts()\n",
    "                       .sort_index().index == pd.to_datetime('2018-9-23'))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rentals[df_rentals.pickup_hub_id == 60].created_at.dt.floor('d').value_counts().sort_index() \\\n",
    ".reindex(complete_index).fillna(value=0)[:summer_start_idx].hist(bins=50)\n",
    "plt.title(\"Pre-Summer Pick-up count distribution in Hub #60 (Norreport Area)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Pick-up count\")\n",
    "plt.xlim([0,18])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rentals[df_rentals.pickup_hub_id == 60].created_at.dt.floor('d').value_counts().sort_index() \\\n",
    ".reindex(complete_index).fillna(value=0)[summer_start_idx:summer_end_idx].hist(bins=50)\n",
    "plt.title(\"Summer Pick-up count distribution in Hub #60 (Norreport Area)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Pick-up count\")\n",
    "plt.xlim([0,18])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rentals[df_rentals.pickup_hub_id == 60].created_at.dt.floor('d').value_counts().sort_index() \\\n",
    ".reindex(complete_index).fillna(value=0)[summer_end_idx:].hist(bins=50)\n",
    "plt.title(\"Post-Summer Pick-up count distribution in Hub #60 (Norreport Area)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Pick-up count\")\n",
    "plt.xlim([0,18])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is therefore reasonable to think that our model should try to extract information from these temporal indicators in our data.\n",
    "\n",
    "It is also important to stress the fact that these plots represent exclusively the pick-up rates for a single hub. We could have chosen to concentrate on the observation of origin-destination rentals (and so focus more on the demand of transportation from Hub A to Hub B rather than the pick-up demand in one hub)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hub availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the specification of our demand prediction problem it is important to notice that the realizations of demand that we observe in every hub are actually *censored*. This basically means that our variable of interest (the demand) has as upper limit the current supply of bikes. For example, suppose that we have a capacity of 10 bikes in one hub, we will never be able to observe a demand of 15, but only a maximum of 10. This behaviour will lead to an observed demand which could be an underestimate of the *real* demand (and therefore to a biased predictor)\n",
    "\n",
    "An indicator of this censoring behaviour in our data could be observed in all those cases where the current availability of the hub reaches a value of 0. Obviously, a hub with no bikes available would not be able to satisfy any existing demand (and we would not be able therefore to observe any rentals).\n",
    "\n",
    "The following plot represents the **minimum value** of bike availability in Hub #60 aggregated to a daily level:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Again, the two following cells give a visual understanding of the preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odered representation of Hub dataset\n",
    "df_hubav[df_hubav.hub_id == 60].sort_values(by='created_at').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum availability for each of the sample days\n",
    "df_hubav[df_hubav.hub_id == 60].groupby(pd.Grouper(key='created_at',freq='d')).min().bike_count_to.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the data, we notice that we have missing availability records for some days. It turns out (as one would expect) that this is due to an unchanged availability in the corresponding Hub. Therefore, in this case it makes complete sense to fill in the missing days with a \"*forward fill*\" method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before fill\n",
    "df_hubav[df_hubav.hub_id == 60].groupby(pd.Grouper(key='created_at',freq='d')).min().bike_count_to.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After fill\n",
    "df_hubav[df_hubav.hub_id == 60].groupby(pd.Grouper(key='created_at',freq='d')).min(). \\\n",
    "bike_count_to.fillna(method='ffill').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to analyse the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hubav[df_hubav.hub_id == 60].groupby(pd.Grouper(key='created_at',freq='d')).min().bike_count_to.fillna(method='ffill').plot()\n",
    "plt.title(\"Minimum Daily Bike Availability in Hub #60 (Norreport Area)\")\n",
    "plt.ylabel(\"Minimum availability\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot clearly shows that (in this specific Hub) it is very common for the bike availability to go to 0 during the day (approximately 69% of the days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searchlogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how our data naturally defines a difference between *observed* and *real* demand. In this perspective, the searchlog data could represent some kind of indicator of the real demand. This data, as we described above, represents the locations of the users in the moment they access the app. It is therefore reasonable to assume that most of the access logs will represent users actually looking for an available bike nearby... \n",
    "\n",
    "To give a visual representation of the searchlogs the following plot represents 1000 searchlogs of 09/03/2019:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_searchlog['date'] = [d.date() for d in df_searchlog['timestamp']]\n",
    "df_search_plot = df_searchlog.where(df_searchlog.date == pd.to_datetime(\"09/03/2019\", format=\"%d/%m/%Y\").date()).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location=initial_location,\n",
    "              zoom_start=13)\n",
    "\n",
    "locations = df_search_plot.iloc[:1000, :2]\n",
    "locationlist = locations.values.tolist()\n",
    "\n",
    "for point in range(0, len(locationlist)):\n",
    "    folium.CircleMarker(\n",
    "    location=locationlist[point],\n",
    "    radius=3,\n",
    "    popup='Searchlog_{}'.format(point),\n",
    "    color='#3186cc',\n",
    "    fill=True,\n",
    "    fill_color='#3186cc').add_to(m)\n",
    "    \n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-specific level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data gives us valuable information on the rental patterns of single users. It could therefore be interesting to try and extrapolate patterns related to single individuals in our dataset. \n",
    "\n",
    "For example, in the following cells we first count how many rentals are recorded for every user (ordering from most active user to less active user). Then, for our plot, we select the most active user and analyse his origin-destination pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rental counts for every user\n",
    "rentals_by_user = df_rentals.groupby('user_id')['user_id'].count().sort_values(ascending=False)\n",
    "rentals_by_user.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trips for user 140961 (most active in our dataset)\n",
    "usr_140961 = df_rentals[df_rentals.user_id == 140961][['pickup_hub_id', 'dropoff_hub_id']].dropna()\n",
    "usr_140961.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualize the first 450 rentals on a map. In particular the blue markers represent the Pick-up locations, while the red markers represent the drop-off locations. \n",
    "\n",
    "Notice how: \n",
    "1. The area of Copenhagen is a pretty well defined subset of the entire city and \n",
    "2. The number of observable hubs is actually way less than 450 (this means that there is a high overlap between the plotted OD pairs - the user has several repetitive trips)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_map = folium.Map(location=initial_location,\n",
    "              zoom_start=13)\n",
    "\n",
    "locations = usr_140961.iloc[:,:]\n",
    "locationlist = locations.values.tolist()\n",
    "\n",
    "for point in range(0, len(locationlist[:450])):\n",
    "    folium.CircleMarker(\n",
    "    location=[df_hubs[df_hubs.id == int(locationlist[point][0])]['latitude'].values[0],df_hubs[df_hubs.id == int(locationlist[point][0])]['longitude'].values[0]],\n",
    "    radius=3,\n",
    "    popup='Rental_{}'.format(point),\n",
    "    color='#3186cc',\n",
    "    fill=True,\n",
    "    fill_color='#3186cc').add_to(user_map)\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "    location=[df_hubs[df_hubs.id == int(locationlist[point][1])]['latitude'].values[0],df_hubs[df_hubs.id == int(locationlist[point][1])]['longitude'].values[0]],\n",
    "    radius=3,\n",
    "    popup='Rental_{}'.format(point),\n",
    "    color='crimson',\n",
    "    fill=True,\n",
    "    fill_color='crimson').add_to(user_map)\n",
    "    \n",
    "user_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
