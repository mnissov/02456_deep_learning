{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import os\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_vector_with_weekday(hubs,cos,sin):\n",
    "    l = []\n",
    "    for hub in range(len(hubs)):\n",
    "        h = list(hubs[hub])\n",
    "        h.extend([sin[hub],cos[hub]])\n",
    "        l.append(np.array(h))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 28, 1551) (10, 1, 1551)\n",
      "(1395, 28, 12) (1395, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_pickle('DR_Data/pickup_series_10max_zones.pickle')\n",
    "pickups_out = np.array(data.Hubs)\n",
    "weekday = True\n",
    "added_features = 0\n",
    "\n",
    "if weekday:\n",
    "    data.Hubs = extend_vector_with_weekday(data.Hubs,data.weekday_sin,data.weekday_cos)\n",
    "    added_features = added_features + 2\n",
    "    \n",
    "pickups_in = np.array(data.Hubs)\n",
    "\n",
    "num_zones = pickups_in[0].shape[0] - added_features # -2 from weekdays sin and cos\n",
    "seq_length = 28\n",
    "num_intervals = pickups_in.shape[0]-seq_length\n",
    "num_features = pickups_in[0].shape[0]\n",
    "validation_size = 0.1\n",
    "\n",
    "X = np.zeros((num_features, seq_length, num_intervals))\n",
    "y = np.zeros((num_zones, 1, num_intervals))\n",
    "for i in range(num_intervals):\n",
    "    X[:,:,i] = np.array(list(pickups_in[i:i+seq_length])).T\n",
    "    y[:,:,i] = np.array([pickups_out[i+seq_length]]).T\n",
    "    \n",
    "print(X.shape,y.shape)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X.T, y.T, test_size=validation_size, random_state=1)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, Dropout2d, MaxPool2d, BatchNorm1d, BatchNorm2d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GPU.\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "#use_cuda =False\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (bn0): BatchNorm1d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (rnn_1): LSTM(12, 16, num_layers=2, dropout=0.5)\n",
      "  (l_out): Linear(in_features=448, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rnn_seq_length = seq_length\n",
    "rnn_input_size = num_features\n",
    "rnn_hidden_size = 16\n",
    "rnn_direction = 0\n",
    "rnn_layers = 2\n",
    "\n",
    "rnn_out_features = rnn_seq_length*rnn_hidden_size*(rnn_direction+1)\n",
    "features_cat_size = rnn_out_features\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()        \n",
    "        #self.drop = Dropout(p=0.5)\n",
    "        self.bn0 = BatchNorm1d(rnn_seq_length)\n",
    "        \n",
    "        self.rnn_1 = nn.LSTM(input_size=rnn_input_size,\n",
    "                            hidden_size=rnn_hidden_size,\n",
    "                            num_layers=rnn_layers,\n",
    "                            bidirectional=(rnn_direction>0),\n",
    "                            dropout=0.5)\n",
    "        \n",
    "        self.l_out = Linear(in_features=features_cat_size,\n",
    "                            out_features=num_zones,\n",
    "                            bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        out = {}\n",
    "        #x = self.bn0(x)\n",
    "        \n",
    "        features_rnn = x.view(rnn_seq_length, -1, rnn_input_size)\n",
    "        features_rnn,_ = self.rnn_1(features_rnn)\n",
    "        #features_rnn = self.drop(features_rnn)\n",
    "        \n",
    "        features_rnn = features_rnn.view(-1, rnn_out_features) #self.rnn_1.hidden_size)\n",
    "        \n",
    "        # Append features to the list \"features\"\n",
    "        features.append(features_rnn)\n",
    "        \n",
    "        ## Output layer where all features are in use ##\n",
    "        features_final = torch.cat(features, dim=1)\n",
    "        \n",
    "        #features_final = self.drop(features_final)\n",
    "        out['out'] = self.l_out(features_final)\n",
    "        '''\n",
    "        x, (h,c) = self.rnn_1(x)\n",
    "        #x = x.view(-1,self.rnn_1.rnn_hidden_size)\n",
    "        x = self.l_out(x)\n",
    "        return x\n",
    "        '''\n",
    "        return out['out']\n",
    "\n",
    "net = Net()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156, 28, 12)\n",
      "Epoch 0 \t train loss: 12.3434626890 \t valid loss: 13.0148048401 \t valid acc: 0.4227\n",
      "Epoch 5 \t train loss: 10.9627798657 \t valid loss: 11.5877821445 \t valid acc: 0.4242\n",
      "Epoch 10 \t train loss: 9.2213425803 \t valid loss: 9.6816856861 \t valid acc: 0.1977\n",
      "Epoch 15 \t train loss: 8.5741094046 \t valid loss: 8.8551368713 \t valid acc: 0.1469\n",
      "Epoch 20 \t train loss: 8.4999211555 \t valid loss: 8.7296581268 \t valid acc: 0.1336\n",
      "Epoch 25 \t train loss: 8.4771340947 \t valid loss: 8.7082539797 \t valid acc: 0.1297\n",
      "Epoch 30 \t train loss: 8.4727326437 \t valid loss: 8.6954855919 \t valid acc: 0.1289\n",
      "Epoch 35 \t train loss: 8.4556183205 \t valid loss: 8.6826483011 \t valid acc: 0.1266\n",
      "Epoch 40 \t train loss: 8.4480491904 \t valid loss: 8.6690151691 \t valid acc: 0.1250\n",
      "Epoch 45 \t train loss: 8.4443492390 \t valid loss: 8.6543456316 \t valid acc: 0.1250\n",
      "Epoch 50 \t train loss: 8.4386819795 \t valid loss: 8.6383278370 \t valid acc: 0.1258\n",
      "Epoch 55 \t train loss: 8.4091983008 \t valid loss: 8.6207451820 \t valid acc: 0.1273\n",
      "Epoch 60 \t train loss: 8.3971193946 \t valid loss: 8.6018270254 \t valid acc: 0.1273\n",
      "Epoch 65 \t train loss: 8.3747237283 \t valid loss: 8.5810636282 \t valid acc: 0.1281\n",
      "Epoch 70 \t train loss: 8.3533171110 \t valid loss: 8.5583170652 \t valid acc: 0.1289\n",
      "Epoch 75 \t train loss: 8.3317505981 \t valid loss: 8.5333031416 \t valid acc: 0.1289\n",
      "Epoch 80 \t train loss: 8.3098781941 \t valid loss: 8.5051695108 \t valid acc: 0.1289\n",
      "Epoch 85 \t train loss: 8.2823368228 \t valid loss: 8.4749265909 \t valid acc: 0.1289\n",
      "Epoch 90 \t train loss: 8.2590796060 \t valid loss: 8.4409481287 \t valid acc: 0.1305\n",
      "Epoch 95 \t train loss: 8.2153836184 \t valid loss: 8.4033472538 \t valid acc: 0.1328\n",
      "Epoch 100 \t train loss: 8.1911675542 \t valid loss: 8.3612188101 \t valid acc: 0.1320\n",
      "Epoch 105 \t train loss: 8.1542603304 \t valid loss: 8.3148612976 \t valid acc: 0.1313\n",
      "Epoch 110 \t train loss: 8.1132126464 \t valid loss: 8.2634731531 \t valid acc: 0.1305\n",
      "Epoch 115 \t train loss: 8.0495867785 \t valid loss: 8.2058103085 \t valid acc: 0.1297\n",
      "Epoch 120 \t train loss: 8.0090506687 \t valid loss: 8.1411601305 \t valid acc: 0.1336\n",
      "Epoch 125 \t train loss: 7.9382504641 \t valid loss: 8.0695260763 \t valid acc: 0.1375\n",
      "Epoch 130 \t train loss: 7.8743036071 \t valid loss: 7.9897080660 \t valid acc: 0.1383\n",
      "Epoch 135 \t train loss: 7.7878693259 \t valid loss: 7.9003336430 \t valid acc: 0.1375\n",
      "Epoch 140 \t train loss: 7.7031352409 \t valid loss: 7.8012467623 \t valid acc: 0.1398\n",
      "Epoch 145 \t train loss: 7.6310361152 \t valid loss: 7.6912363768 \t valid acc: 0.1383\n",
      "Epoch 150 \t train loss: 7.5198229579 \t valid loss: 7.5714162588 \t valid acc: 0.1391\n",
      "Epoch 155 \t train loss: 7.3967572645 \t valid loss: 7.4401131868 \t valid acc: 0.1383\n",
      "Epoch 160 \t train loss: 7.2743633514 \t valid loss: 7.2980551720 \t valid acc: 0.1383\n",
      "Epoch 165 \t train loss: 7.1513779385 \t valid loss: 7.1464741230 \t valid acc: 0.1391\n",
      "Epoch 170 \t train loss: 7.0161357924 \t valid loss: 6.9876992702 \t valid acc: 0.1398\n",
      "Epoch 175 \t train loss: 6.8585560599 \t valid loss: 6.8223392963 \t valid acc: 0.1430\n",
      "Epoch 180 \t train loss: 6.7451390111 \t valid loss: 6.6535773277 \t valid acc: 0.1500\n",
      "Epoch 185 \t train loss: 6.6040177900 \t valid loss: 6.4862043858 \t valid acc: 0.1633\n",
      "Epoch 190 \t train loss: 6.4586172048 \t valid loss: 6.3223315477 \t valid acc: 0.1664\n",
      "Epoch 195 \t train loss: 6.2998102876 \t valid loss: 6.1635607481 \t valid acc: 0.1656\n",
      "Epoch 200 \t train loss: 6.1635226982 \t valid loss: 6.0147551298 \t valid acc: 0.1844\n",
      "Epoch 205 \t train loss: 6.0505852422 \t valid loss: 5.8757314682 \t valid acc: 0.1906\n",
      "Epoch 210 \t train loss: 5.9982989611 \t valid loss: 5.7491651773 \t valid acc: 0.1922\n",
      "Epoch 215 \t train loss: 5.8820987302 \t valid loss: 5.6368167400 \t valid acc: 0.2086\n",
      "Epoch 220 \t train loss: 5.7839246850 \t valid loss: 5.5378170013 \t valid acc: 0.2266\n",
      "Epoch 225 \t train loss: 5.6844568308 \t valid loss: 5.4496174455 \t valid acc: 0.2344\n",
      "Epoch 230 \t train loss: 5.6274040023 \t valid loss: 5.3720305562 \t valid acc: 0.2500\n",
      "Epoch 235 \t train loss: 5.6002039189 \t valid loss: 5.3056508899 \t valid acc: 0.2586\n",
      "Epoch 240 \t train loss: 5.5166256816 \t valid loss: 5.2509561777 \t valid acc: 0.2734\n",
      "Epoch 245 \t train loss: 5.4724853704 \t valid loss: 5.2013483644 \t valid acc: 0.2805\n",
      "Epoch 250 \t train loss: 5.4083375820 \t valid loss: 5.1593060493 \t valid acc: 0.2875\n",
      "Epoch 255 \t train loss: 5.4036833297 \t valid loss: 5.1238061786 \t valid acc: 0.3063\n",
      "Epoch 260 \t train loss: 5.3506578346 \t valid loss: 5.0916293859 \t valid acc: 0.3133\n",
      "Epoch 265 \t train loss: 5.3404243990 \t valid loss: 5.0638139844 \t valid acc: 0.3148\n",
      "Epoch 270 \t train loss: 5.2640972082 \t valid loss: 5.0374665260 \t valid acc: 0.3148\n",
      "Epoch 275 \t train loss: 5.2589665790 \t valid loss: 5.0169053674 \t valid acc: 0.3195\n",
      "Epoch 280 \t train loss: 5.2464066162 \t valid loss: 4.9973499775 \t valid acc: 0.3164\n",
      "Epoch 285 \t train loss: 5.2368069416 \t valid loss: 4.9798403382 \t valid acc: 0.3180\n",
      "Epoch 290 \t train loss: 5.2020741452 \t valid loss: 4.9654554129 \t valid acc: 0.3211\n",
      "Epoch 295 \t train loss: 5.2016444705 \t valid loss: 4.9516891837 \t valid acc: 0.3219\n",
      "Epoch 300 \t train loss: 5.1854882739 \t valid loss: 4.9390228987 \t valid acc: 0.3234\n",
      "Epoch 305 \t train loss: 5.1859492867 \t valid loss: 4.9257692695 \t valid acc: 0.3250\n",
      "Epoch 310 \t train loss: 5.1120560114 \t valid loss: 4.9149819016 \t valid acc: 0.3273\n",
      "Epoch 315 \t train loss: 5.1334569343 \t valid loss: 4.9027068615 \t valid acc: 0.3258\n",
      "Epoch 320 \t train loss: 5.1008864669 \t valid loss: 4.8916000724 \t valid acc: 0.3313\n",
      "Epoch 325 \t train loss: 5.1140180965 \t valid loss: 4.8786835670 \t valid acc: 0.3313\n",
      "Epoch 330 \t train loss: 5.1244720248 \t valid loss: 4.8712980747 \t valid acc: 0.3320\n",
      "Epoch 335 \t train loss: 5.0688989551 \t valid loss: 4.8575662374 \t valid acc: 0.3281\n",
      "Epoch 340 \t train loss: 5.0768781263 \t valid loss: 4.8468233347 \t valid acc: 0.3273\n",
      "Epoch 345 \t train loss: 5.0508617412 \t valid loss: 4.8348855972 \t valid acc: 0.3289\n",
      "Epoch 350 \t train loss: 5.0294965811 \t valid loss: 4.8222472072 \t valid acc: 0.3289\n",
      "Epoch 355 \t train loss: 5.0429819739 \t valid loss: 4.8083717823 \t valid acc: 0.3258\n",
      "Epoch 360 \t train loss: 5.0138677165 \t valid loss: 4.7964476347 \t valid acc: 0.3242\n",
      "Epoch 365 \t train loss: 4.9772882628 \t valid loss: 4.7842912078 \t valid acc: 0.3258\n",
      "Epoch 370 \t train loss: 4.9833271670 \t valid loss: 4.7744374275 \t valid acc: 0.3258\n",
      "Epoch 375 \t train loss: 5.0570444950 \t valid loss: 4.7658001184 \t valid acc: 0.3266\n",
      "Epoch 380 \t train loss: 5.0407537527 \t valid loss: 4.7591621876 \t valid acc: 0.3289\n",
      "Epoch 385 \t train loss: 4.9531575580 \t valid loss: 4.7469851971 \t valid acc: 0.3305\n",
      "Epoch 390 \t train loss: 4.9475295377 \t valid loss: 4.7351713181 \t valid acc: 0.3242\n",
      "Epoch 395 \t train loss: 4.9935172214 \t valid loss: 4.7236658931 \t valid acc: 0.3289\n",
      "Epoch 400 \t train loss: 4.9862140777 \t valid loss: 4.7112843394 \t valid acc: 0.3258\n",
      "Epoch 405 \t train loss: 4.9956732240 \t valid loss: 4.7007846832 \t valid acc: 0.3320\n",
      "Epoch 410 \t train loss: 4.9306225555 \t valid loss: 4.6909105778 \t valid acc: 0.3281\n",
      "Epoch 415 \t train loss: 4.8728559516 \t valid loss: 4.6826775074 \t valid acc: 0.3273\n",
      "Epoch 420 \t train loss: 4.9185427178 \t valid loss: 4.6726371646 \t valid acc: 0.3336\n",
      "Epoch 425 \t train loss: 4.9188013409 \t valid loss: 4.6646507978 \t valid acc: 0.3352\n",
      "Epoch 430 \t train loss: 4.9040888964 \t valid loss: 4.6588499546 \t valid acc: 0.3297\n",
      "Epoch 435 \t train loss: 4.9174815222 \t valid loss: 4.6524059772 \t valid acc: 0.3281\n",
      "Epoch 440 \t train loss: 4.9146581417 \t valid loss: 4.6413525343 \t valid acc: 0.3305\n",
      "Epoch 445 \t train loss: 4.8975272012 \t valid loss: 4.6360129714 \t valid acc: 0.3250\n",
      "Epoch 450 \t train loss: 4.9307184996 \t valid loss: 4.6257997751 \t valid acc: 0.3250\n",
      "Epoch 455 \t train loss: 4.8942806277 \t valid loss: 4.6200563908 \t valid acc: 0.3227\n",
      "Epoch 460 \t train loss: 4.8617247482 \t valid loss: 4.6125050783 \t valid acc: 0.3297\n",
      "Epoch 465 \t train loss: 4.8618200213 \t valid loss: 4.6039647460 \t valid acc: 0.3328\n",
      "Epoch 470 \t train loss: 4.8807499520 \t valid loss: 4.6020507812 \t valid acc: 0.3266\n",
      "Epoch 475 \t train loss: 4.8875819417 \t valid loss: 4.5924015641 \t valid acc: 0.3344\n",
      "Epoch 480 \t train loss: 4.8418385816 \t valid loss: 4.5882076025 \t valid acc: 0.3336\n",
      "Epoch 485 \t train loss: 4.8477428514 \t valid loss: 4.5839584470 \t valid acc: 0.3336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 490 \t train loss: 4.8324009041 \t valid loss: 4.5776026249 \t valid acc: 0.3336\n",
      "Epoch 495 \t train loss: 4.8120252698 \t valid loss: 4.5715532303 \t valid acc: 0.3344\n",
      "Epoch 500 \t train loss: 4.8482732218 \t valid loss: 4.5627725720 \t valid acc: 0.3344\n",
      "Epoch 505 \t train loss: 4.8047176350 \t valid loss: 4.5551797748 \t valid acc: 0.3359\n",
      "Epoch 510 \t train loss: 4.8391234431 \t valid loss: 4.5487131476 \t valid acc: 0.3359\n",
      "Epoch 515 \t train loss: 4.8412802219 \t valid loss: 4.5445135832 \t valid acc: 0.3328\n",
      "Epoch 520 \t train loss: 4.8474614620 \t valid loss: 4.5375386477 \t valid acc: 0.3367\n",
      "Epoch 525 \t train loss: 4.7855143270 \t valid loss: 4.5313993096 \t valid acc: 0.3359\n",
      "Epoch 530 \t train loss: 4.7660782282 \t valid loss: 4.5267505646 \t valid acc: 0.3375\n",
      "Epoch 535 \t train loss: 4.7686104719 \t valid loss: 4.5219354033 \t valid acc: 0.3406\n",
      "Epoch 540 \t train loss: 4.7903076992 \t valid loss: 4.5182954669 \t valid acc: 0.3375\n",
      "Epoch 545 \t train loss: 4.7922909814 \t valid loss: 4.5223445296 \t valid acc: 0.3273\n",
      "Epoch 550 \t train loss: 4.7754457108 \t valid loss: 4.5113048553 \t valid acc: 0.3352\n",
      "Epoch 555 \t train loss: 4.7712521553 \t valid loss: 4.5072446465 \t valid acc: 0.3367\n",
      "Epoch 560 \t train loss: 4.8004659276 \t valid loss: 4.5008023977 \t valid acc: 0.3375\n",
      "Epoch 565 \t train loss: 4.8125741205 \t valid loss: 4.4981502295 \t valid acc: 0.3383\n",
      "Epoch 570 \t train loss: 4.7846723102 \t valid loss: 4.4918493629 \t valid acc: 0.3375\n",
      "Epoch 575 \t train loss: 4.7660340653 \t valid loss: 4.4931473136 \t valid acc: 0.3297\n",
      "Epoch 580 \t train loss: 4.7268870210 \t valid loss: 4.4880794883 \t valid acc: 0.3320\n",
      "Epoch 585 \t train loss: 4.7466054573 \t valid loss: 4.4863833785 \t valid acc: 0.3281\n",
      "Epoch 590 \t train loss: 4.7099629557 \t valid loss: 4.4782660604 \t valid acc: 0.3297\n",
      "Epoch 595 \t train loss: 4.7530669667 \t valid loss: 4.4781524539 \t valid acc: 0.3305\n",
      "Epoch 600 \t train loss: 4.7488945473 \t valid loss: 4.4757223129 \t valid acc: 0.3297\n",
      "Epoch 605 \t train loss: 4.7488774311 \t valid loss: 4.4635252357 \t valid acc: 0.3313\n",
      "Epoch 610 \t train loss: 4.7892715986 \t valid loss: 4.4610498548 \t valid acc: 0.3320\n",
      "Epoch 615 \t train loss: 4.7451693679 \t valid loss: 4.4524765611 \t valid acc: 0.3328\n",
      "Epoch 620 \t train loss: 4.7144632672 \t valid loss: 4.4562764764 \t valid acc: 0.3328\n",
      "Epoch 625 \t train loss: 4.7301233258 \t valid loss: 4.4493481517 \t valid acc: 0.3336\n",
      "Epoch 630 \t train loss: 4.6792280729 \t valid loss: 4.4433376193 \t valid acc: 0.3344\n",
      "Epoch 635 \t train loss: 4.7115096270 \t valid loss: 4.4429272413 \t valid acc: 0.3336\n",
      "Epoch 640 \t train loss: 4.7154759584 \t valid loss: 4.4419844747 \t valid acc: 0.3344\n",
      "Epoch 645 \t train loss: 4.6907704764 \t valid loss: 4.4354737401 \t valid acc: 0.3391\n",
      "Epoch 650 \t train loss: 4.7187344141 \t valid loss: 4.4346606135 \t valid acc: 0.3398\n",
      "Epoch 655 \t train loss: 4.7162719494 \t valid loss: 4.4306738973 \t valid acc: 0.3422\n",
      "Epoch 660 \t train loss: 4.6588746448 \t valid loss: 4.4248043299 \t valid acc: 0.3375\n",
      "Epoch 665 \t train loss: 4.7114524564 \t valid loss: 4.4264568090 \t valid acc: 0.3367\n",
      "Epoch 670 \t train loss: 4.6850307376 \t valid loss: 4.4238655567 \t valid acc: 0.3367\n",
      "Epoch 675 \t train loss: 4.7235101323 \t valid loss: 4.4205727577 \t valid acc: 0.3359\n",
      "Epoch 680 \t train loss: 4.6768607650 \t valid loss: 4.4192444682 \t valid acc: 0.3391\n",
      "Epoch 685 \t train loss: 4.6926480892 \t valid loss: 4.4135248661 \t valid acc: 0.3406\n",
      "Epoch 690 \t train loss: 4.6678197273 \t valid loss: 4.4070458412 \t valid acc: 0.3398\n",
      "Epoch 695 \t train loss: 4.6846432187 \t valid loss: 4.4024601579 \t valid acc: 0.3406\n",
      "Epoch 700 \t train loss: 4.7139720917 \t valid loss: 4.4029866457 \t valid acc: 0.3406\n",
      "Epoch 705 \t train loss: 4.6751465576 \t valid loss: 4.4020009041 \t valid acc: 0.3430\n",
      "Epoch 710 \t train loss: 4.6393583431 \t valid loss: 4.3923521638 \t valid acc: 0.3422\n",
      "Epoch 715 \t train loss: 4.6326854229 \t valid loss: 4.3887659907 \t valid acc: 0.3414\n",
      "Epoch 720 \t train loss: 4.6139749760 \t valid loss: 4.3933111429 \t valid acc: 0.3422\n",
      "Epoch 725 \t train loss: 4.6482182713 \t valid loss: 4.3948469162 \t valid acc: 0.3391\n",
      "Epoch 730 \t train loss: 4.6320778048 \t valid loss: 4.3901016116 \t valid acc: 0.3398\n",
      "Epoch 735 \t train loss: 4.6252595990 \t valid loss: 4.3885592222 \t valid acc: 0.3438\n",
      "Epoch 740 \t train loss: 4.6023542437 \t valid loss: 4.3867939711 \t valid acc: 0.3375\n",
      "Epoch 745 \t train loss: 4.6561002898 \t valid loss: 4.3920829892 \t valid acc: 0.3406\n",
      "Epoch 750 \t train loss: 4.6302470496 \t valid loss: 4.3837081790 \t valid acc: 0.3406\n",
      "Epoch 755 \t train loss: 4.6268678710 \t valid loss: 4.3827942014 \t valid acc: 0.3391\n",
      "Epoch 760 \t train loss: 4.6187187905 \t valid loss: 4.3810687661 \t valid acc: 0.3391\n",
      "Epoch 765 \t train loss: 4.6445970203 \t valid loss: 4.3765631914 \t valid acc: 0.3367\n",
      "Epoch 770 \t train loss: 4.6555665648 \t valid loss: 4.3788304925 \t valid acc: 0.3422\n",
      "Epoch 775 \t train loss: 4.6004862785 \t valid loss: 4.3768157959 \t valid acc: 0.3375\n",
      "Epoch 780 \t train loss: 4.5993413592 \t valid loss: 4.3734868765 \t valid acc: 0.3391\n",
      "Epoch 785 \t train loss: 4.6159589235 \t valid loss: 4.3703466058 \t valid acc: 0.3383\n",
      "Epoch 790 \t train loss: 4.5969702809 \t valid loss: 4.3654859066 \t valid acc: 0.3344\n",
      "Epoch 795 \t train loss: 4.5999288282 \t valid loss: 4.3631914854 \t valid acc: 0.3359\n",
      "Epoch 800 \t train loss: 4.6107402680 \t valid loss: 4.3637595177 \t valid acc: 0.3375\n",
      "Epoch 805 \t train loss: 4.5934280407 \t valid loss: 4.3625532389 \t valid acc: 0.3359\n",
      "Epoch 810 \t train loss: 4.6094237427 \t valid loss: 4.3675987720 \t valid acc: 0.3359\n",
      "Epoch 815 \t train loss: 4.6013991944 \t valid loss: 4.3622844219 \t valid acc: 0.3344\n",
      "Epoch 820 \t train loss: 4.6272396986 \t valid loss: 4.3617069721 \t valid acc: 0.3336\n",
      "Epoch 825 \t train loss: 4.5839169081 \t valid loss: 4.3693529963 \t valid acc: 0.3359\n",
      "Epoch 830 \t train loss: 4.5747315052 \t valid loss: 4.3644229174 \t valid acc: 0.3391\n",
      "Epoch 835 \t train loss: 4.6453663083 \t valid loss: 4.3612887859 \t valid acc: 0.3375\n",
      "Epoch 840 \t train loss: 4.6238345213 \t valid loss: 4.3692126274 \t valid acc: 0.3313\n",
      "Epoch 845 \t train loss: 4.6404976623 \t valid loss: 4.3584967256 \t valid acc: 0.3375\n",
      "Epoch 850 \t train loss: 4.5566384515 \t valid loss: 4.3607722521 \t valid acc: 0.3352\n",
      "Epoch 855 \t train loss: 4.5634869587 \t valid loss: 4.3559589386 \t valid acc: 0.3414\n",
      "Epoch 860 \t train loss: 4.5637222113 \t valid loss: 4.3531481624 \t valid acc: 0.3414\n",
      "Epoch 865 \t train loss: 4.5963277151 \t valid loss: 4.3550339937 \t valid acc: 0.3406\n",
      "Epoch 870 \t train loss: 4.5403654686 \t valid loss: 4.3491200805 \t valid acc: 0.3422\n",
      "Epoch 875 \t train loss: 4.6009026350 \t valid loss: 4.3495026231 \t valid acc: 0.3438\n",
      "Epoch 880 \t train loss: 4.5910748216 \t valid loss: 4.3481937647 \t valid acc: 0.3359\n",
      "Epoch 885 \t train loss: 4.5598033639 \t valid loss: 4.3452990055 \t valid acc: 0.3414\n",
      "Epoch 890 \t train loss: 4.5910298048 \t valid loss: 4.3423895836 \t valid acc: 0.3406\n",
      "Epoch 895 \t train loss: 4.5737804036 \t valid loss: 4.3499227762 \t valid acc: 0.3367\n",
      "Epoch 900 \t train loss: 4.6037475508 \t valid loss: 4.3457842469 \t valid acc: 0.3430\n",
      "Epoch 905 \t train loss: 4.5644187595 \t valid loss: 4.3426409364 \t valid acc: 0.3438\n",
      "Epoch 910 \t train loss: 4.5463267925 \t valid loss: 4.3466961384 \t valid acc: 0.3398\n",
      "Epoch 915 \t train loss: 4.5929328397 \t valid loss: 4.3468770385 \t valid acc: 0.3414\n",
      "Epoch 920 \t train loss: 4.5450384950 \t valid loss: 4.3446550965 \t valid acc: 0.3453\n",
      "Epoch 925 \t train loss: 4.5348483463 \t valid loss: 4.3425318599 \t valid acc: 0.3414\n",
      "Epoch 930 \t train loss: 4.5293017986 \t valid loss: 4.3437794447 \t valid acc: 0.3453\n",
      "Epoch 935 \t train loss: 4.5558644949 \t valid loss: 4.3433209062 \t valid acc: 0.3438\n",
      "Epoch 940 \t train loss: 4.5718563047 \t valid loss: 4.3484181166 \t valid acc: 0.3383\n",
      "Epoch 945 \t train loss: 4.5527339647 \t valid loss: 4.3451092839 \t valid acc: 0.3453\n",
      "Epoch 950 \t train loss: 4.5421965233 \t valid loss: 4.3477048874 \t valid acc: 0.3391\n",
      "Epoch 955 \t train loss: 4.5467156699 \t valid loss: 4.3480345011 \t valid acc: 0.3453\n",
      "Epoch 960 \t train loss: 4.5527762590 \t valid loss: 4.3476620317 \t valid acc: 0.3391\n",
      "Epoch 965 \t train loss: 4.5644048480 \t valid loss: 4.3487071991 \t valid acc: 0.3484\n",
      "Epoch 970 \t train loss: 4.5462227089 \t valid loss: 4.3424654603 \t valid acc: 0.3461\n",
      "Epoch 975 \t train loss: 4.5523358168 \t valid loss: 4.3411346674 \t valid acc: 0.3438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 980 \t train loss: 4.5298882695 \t valid loss: 4.3409779072 \t valid acc: 0.3453\n",
      "Epoch 985 \t train loss: 4.5251233356 \t valid loss: 4.3412507176 \t valid acc: 0.3477\n",
      "Epoch 990 \t train loss: 4.5592420268 \t valid loss: 4.3453749418 \t valid acc: 0.3453\n",
      "Epoch 995 \t train loss: 4.5304465904 \t valid loss: 4.3472608924 \t valid acc: 0.3477\n",
      "Epoch 1000 \t train loss: 4.5557097612 \t valid loss: 4.3415541649 \t valid acc: 0.3477\n",
      "Epoch 1005 \t train loss: 4.5166178748 \t valid loss: 4.3389524221 \t valid acc: 0.3406\n",
      "Epoch 1010 \t train loss: 4.5287989406 \t valid loss: 4.3365822434 \t valid acc: 0.3461\n",
      "Epoch 1015 \t train loss: 4.4981802785 \t valid loss: 4.3335049748 \t valid acc: 0.3477\n",
      "Epoch 1020 \t train loss: 4.5442878867 \t valid loss: 4.3347629309 \t valid acc: 0.3445\n",
      "Epoch 1025 \t train loss: 4.5550735496 \t valid loss: 4.3330408931 \t valid acc: 0.3477\n",
      "Epoch 1030 \t train loss: 4.5435920926 \t valid loss: 4.3314006329 \t valid acc: 0.3438\n",
      "Epoch 1035 \t train loss: 4.4830773930 \t valid loss: 4.3309965730 \t valid acc: 0.3453\n",
      "Epoch 1040 \t train loss: 4.5687672316 \t valid loss: 4.3313702345 \t valid acc: 0.3461\n",
      "Epoch 1045 \t train loss: 4.5162801354 \t valid loss: 4.3350558877 \t valid acc: 0.3430\n",
      "Epoch 1050 \t train loss: 4.5101851308 \t valid loss: 4.3321596384 \t valid acc: 0.3445\n",
      "Epoch 1055 \t train loss: 4.5416078845 \t valid loss: 4.3315241933 \t valid acc: 0.3484\n",
      "Epoch 1060 \t train loss: 4.5671681138 \t valid loss: 4.3334314823 \t valid acc: 0.3445\n",
      "Epoch 1065 \t train loss: 4.5293197244 \t valid loss: 4.3345129490 \t valid acc: 0.3492\n",
      "Epoch 1070 \t train loss: 4.5449248525 \t valid loss: 4.3369628191 \t valid acc: 0.3422\n",
      "Epoch 1075 \t train loss: 4.5388746372 \t valid loss: 4.3371381760 \t valid acc: 0.3438\n",
      "Epoch 1080 \t train loss: 4.5012989765 \t valid loss: 4.3330863714 \t valid acc: 0.3422\n",
      "Epoch 1085 \t train loss: 4.5629419837 \t valid loss: 4.3343067169 \t valid acc: 0.3430\n",
      "Epoch 1090 \t train loss: 4.4858965430 \t valid loss: 4.3328180909 \t valid acc: 0.3445\n",
      "Epoch 1095 \t train loss: 4.5115611997 \t valid loss: 4.3336648345 \t valid acc: 0.3477\n",
      "Epoch 1100 \t train loss: 4.5138731003 \t valid loss: 4.3388533592 \t valid acc: 0.3375\n",
      "Epoch 1105 \t train loss: 4.5331582302 \t valid loss: 4.3324562907 \t valid acc: 0.3422\n",
      "Epoch 1110 \t train loss: 4.5359067473 \t valid loss: 4.3288000226 \t valid acc: 0.3445\n",
      "Epoch 1115 \t train loss: 4.5201247570 \t valid loss: 4.3318269849 \t valid acc: 0.3445\n",
      "Epoch 1120 \t train loss: 4.5054824186 \t valid loss: 4.3273668289 \t valid acc: 0.3445\n",
      "Epoch 1125 \t train loss: 4.5209434199 \t valid loss: 4.3250567317 \t valid acc: 0.3406\n",
      "Epoch 1130 \t train loss: 4.5483285106 \t valid loss: 4.3260549903 \t valid acc: 0.3375\n",
      "Epoch 1135 \t train loss: 4.5318564759 \t valid loss: 4.3201391697 \t valid acc: 0.3375\n",
      "Epoch 1140 \t train loss: 4.5190847086 \t valid loss: 4.3199020624 \t valid acc: 0.3414\n",
      "Epoch 1145 \t train loss: 4.5310016787 \t valid loss: 4.3254091144 \t valid acc: 0.3383\n",
      "Epoch 1150 \t train loss: 4.4620436957 \t valid loss: 4.3185298443 \t valid acc: 0.3453\n",
      "Epoch 1155 \t train loss: 4.4683996023 \t valid loss: 4.3192397952 \t valid acc: 0.3375\n",
      "Epoch 1160 \t train loss: 4.4954517498 \t valid loss: 4.3152586818 \t valid acc: 0.3469\n",
      "Epoch 1165 \t train loss: 4.5195320651 \t valid loss: 4.3236729503 \t valid acc: 0.3375\n",
      "Epoch 1170 \t train loss: 4.4768481088 \t valid loss: 4.3191651702 \t valid acc: 0.3438\n",
      "Epoch 1175 \t train loss: 4.4515974411 \t valid loss: 4.3194032907 \t valid acc: 0.3406\n",
      "Epoch 1180 \t train loss: 4.5054215830 \t valid loss: 4.3206561208 \t valid acc: 0.3383\n",
      "Epoch 1185 \t train loss: 4.4962417747 \t valid loss: 4.3229296207 \t valid acc: 0.3398\n",
      "Epoch 1190 \t train loss: 4.4973915233 \t valid loss: 4.3191490173 \t valid acc: 0.3453\n",
      "Epoch 1195 \t train loss: 4.5342554603 \t valid loss: 4.3197537661 \t valid acc: 0.3367\n",
      "Epoch 1200 \t train loss: 4.4702575151 \t valid loss: 4.3205263019 \t valid acc: 0.3391\n",
      "Epoch 1205 \t train loss: 4.4716162183 \t valid loss: 4.3196702003 \t valid acc: 0.3445\n",
      "Epoch 1210 \t train loss: 4.4991961025 \t valid loss: 4.3253696561 \t valid acc: 0.3406\n",
      "Epoch 1215 \t train loss: 4.4976562178 \t valid loss: 4.3216212392 \t valid acc: 0.3383\n",
      "Epoch 1220 \t train loss: 4.5204397135 \t valid loss: 4.3171834946 \t valid acc: 0.3367\n",
      "Epoch 1225 \t train loss: 4.4730043023 \t valid loss: 4.3151838779 \t valid acc: 0.3430\n",
      "Epoch 1230 \t train loss: 4.4813028103 \t valid loss: 4.3176353574 \t valid acc: 0.3398\n",
      "Epoch 1235 \t train loss: 4.5037466703 \t valid loss: 4.3176344037 \t valid acc: 0.3391\n",
      "Epoch 1240 \t train loss: 4.4698437924 \t valid loss: 4.3169339895 \t valid acc: 0.3367\n",
      "Epoch 1245 \t train loss: 4.4799278614 \t valid loss: 4.3132952452 \t valid acc: 0.3406\n",
      "Epoch 1250 \t train loss: 4.4864391671 \t valid loss: 4.3182981014 \t valid acc: 0.3359\n",
      "Epoch 1255 \t train loss: 4.4819136719 \t valid loss: 4.3089842796 \t valid acc: 0.3422\n",
      "Epoch 1260 \t train loss: 4.5104762399 \t valid loss: 4.3100133538 \t valid acc: 0.3430\n",
      "Epoch 1265 \t train loss: 4.4854247071 \t valid loss: 4.3119763732 \t valid acc: 0.3367\n",
      "Epoch 1270 \t train loss: 4.4471867916 \t valid loss: 4.3093404770 \t valid acc: 0.3391\n",
      "Epoch 1275 \t train loss: 4.4423286638 \t valid loss: 4.3049031496 \t valid acc: 0.3414\n",
      "Epoch 1280 \t train loss: 4.4681776679 \t valid loss: 4.3123235106 \t valid acc: 0.3398\n",
      "Epoch 1285 \t train loss: 4.4606438182 \t valid loss: 4.3106858730 \t valid acc: 0.3406\n",
      "Epoch 1290 \t train loss: 4.4686102701 \t valid loss: 4.3085063100 \t valid acc: 0.3406\n",
      "Epoch 1295 \t train loss: 4.4835447322 \t valid loss: 4.3097043633 \t valid acc: 0.3414\n",
      "Epoch 1300 \t train loss: 4.4769863084 \t valid loss: 4.3149360418 \t valid acc: 0.3422\n",
      "Epoch 1305 \t train loss: 4.4742148144 \t valid loss: 4.3085078597 \t valid acc: 0.3453\n",
      "Epoch 1310 \t train loss: 4.4489428942 \t valid loss: 4.3157784343 \t valid acc: 0.3422\n",
      "Epoch 1315 \t train loss: 4.4239742257 \t valid loss: 4.3182017803 \t valid acc: 0.3445\n",
      "Epoch 1320 \t train loss: 4.4746072514 \t valid loss: 4.3191074729 \t valid acc: 0.3414\n",
      "Epoch 1325 \t train loss: 4.4799209362 \t valid loss: 4.3183094859 \t valid acc: 0.3445\n",
      "Epoch 1330 \t train loss: 4.4816584975 \t valid loss: 4.3125356436 \t valid acc: 0.3430\n",
      "Epoch 1335 \t train loss: 4.4417888065 \t valid loss: 4.3097394705 \t valid acc: 0.3453\n",
      "Epoch 1340 \t train loss: 4.4875628061 \t valid loss: 4.3093663454 \t valid acc: 0.3453\n",
      "Epoch 1345 \t train loss: 4.4486063691 \t valid loss: 4.3083844781 \t valid acc: 0.3438\n",
      "Epoch 1350 \t train loss: 4.4783372879 \t valid loss: 4.3081336021 \t valid acc: 0.3453\n",
      "Epoch 1355 \t train loss: 4.4575870980 \t valid loss: 4.3080617785 \t valid acc: 0.3422\n",
      "Epoch 1360 \t train loss: 4.4672063894 \t valid loss: 4.3053149581 \t valid acc: 0.3438\n",
      "Epoch 1365 \t train loss: 4.4904915011 \t valid loss: 4.3037473559 \t valid acc: 0.3445\n",
      "Epoch 1370 \t train loss: 4.4830362076 \t valid loss: 4.3035472035 \t valid acc: 0.3445\n",
      "Epoch 1375 \t train loss: 4.4505504619 \t valid loss: 4.3023024797 \t valid acc: 0.3438\n",
      "Epoch 1380 \t train loss: 4.4824004672 \t valid loss: 4.3032976389 \t valid acc: 0.3469\n",
      "Epoch 1385 \t train loss: 4.4463087348 \t valid loss: 4.3037335277 \t valid acc: 0.3492\n",
      "Epoch 1390 \t train loss: 4.4548958845 \t valid loss: 4.3063327074 \t valid acc: 0.3445\n",
      "Epoch 1395 \t train loss: 4.4532503583 \t valid loss: 4.2994025946 \t valid acc: 0.3500\n",
      "Epoch 1400 \t train loss: 4.4275956209 \t valid loss: 4.3018192649 \t valid acc: 0.3453\n",
      "Epoch 1405 \t train loss: 4.4756185731 \t valid loss: 4.2988833785 \t valid acc: 0.3461\n",
      "Epoch 1410 \t train loss: 4.4882131177 \t valid loss: 4.2979528308 \t valid acc: 0.3461\n",
      "Epoch 1415 \t train loss: 4.4152341166 \t valid loss: 4.3028630614 \t valid acc: 0.3453\n",
      "Epoch 1420 \t train loss: 4.3873980822 \t valid loss: 4.2987551689 \t valid acc: 0.3484\n",
      "Epoch 1425 \t train loss: 4.4330223050 \t valid loss: 4.3000972867 \t valid acc: 0.3461\n",
      "Epoch 1430 \t train loss: 4.4953985048 \t valid loss: 4.2967540622 \t valid acc: 0.3477\n",
      "Epoch 1435 \t train loss: 4.4347812797 \t valid loss: 4.2946969867 \t valid acc: 0.3477\n",
      "Epoch 1440 \t train loss: 4.4547656026 \t valid loss: 4.2945581675 \t valid acc: 0.3453\n",
      "Epoch 1445 \t train loss: 4.4603236442 \t valid loss: 4.2949821353 \t valid acc: 0.3438\n",
      "Epoch 1450 \t train loss: 4.4540640421 \t valid loss: 4.2974383831 \t valid acc: 0.3492\n",
      "Epoch 1455 \t train loss: 4.4499589953 \t valid loss: 4.2931934595 \t valid acc: 0.3492\n",
      "Epoch 1460 \t train loss: 4.4013512412 \t valid loss: 4.2939968705 \t valid acc: 0.3477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1465 \t train loss: 4.4874575582 \t valid loss: 4.3005818129 \t valid acc: 0.3531\n",
      "Epoch 1470 \t train loss: 4.4169881677 \t valid loss: 4.2960969806 \t valid acc: 0.3484\n",
      "Epoch 1475 \t train loss: 4.4159444487 \t valid loss: 4.2973921299 \t valid acc: 0.3516\n",
      "Epoch 1480 \t train loss: 4.4342732984 \t valid loss: 4.2963302135 \t valid acc: 0.3508\n",
      "Epoch 1485 \t train loss: 4.4077809023 \t valid loss: 4.2964328527 \t valid acc: 0.3477\n",
      "Epoch 1490 \t train loss: 4.4742209800 \t valid loss: 4.2941518426 \t valid acc: 0.3508\n",
      "Epoch 1495 \t train loss: 4.4318601863 \t valid loss: 4.2919441462 \t valid acc: 0.3484\n",
      "Epoch 1500 \t train loss: 4.4188400978 \t valid loss: 4.3000716567 \t valid acc: 0.3492\n",
      "Epoch 1505 \t train loss: 4.4484747676 \t valid loss: 4.2961360216 \t valid acc: 0.3531\n",
      "Epoch 1510 \t train loss: 4.4268323876 \t valid loss: 4.2952976227 \t valid acc: 0.3461\n",
      "Epoch 1515 \t train loss: 4.4351542717 \t valid loss: 4.2918362617 \t valid acc: 0.3539\n",
      "Epoch 1520 \t train loss: 4.3980761018 \t valid loss: 4.2875115275 \t valid acc: 0.3516\n",
      "Epoch 1525 \t train loss: 4.4465871744 \t valid loss: 4.2863128185 \t valid acc: 0.3523\n",
      "Epoch 1530 \t train loss: 4.4108884057 \t valid loss: 4.2824228406 \t valid acc: 0.3531\n",
      "Epoch 1535 \t train loss: 4.4273148526 \t valid loss: 4.2862139940 \t valid acc: 0.3531\n",
      "Epoch 1540 \t train loss: 4.4048564157 \t valid loss: 4.2892574072 \t valid acc: 0.3508\n",
      "Epoch 1545 \t train loss: 4.4775082344 \t valid loss: 4.2870204449 \t valid acc: 0.3500\n",
      "Epoch 1550 \t train loss: 4.4315651073 \t valid loss: 4.2866247892 \t valid acc: 0.3484\n",
      "Epoch 1555 \t train loss: 4.4150849664 \t valid loss: 4.2885577083 \t valid acc: 0.3508\n",
      "Epoch 1560 \t train loss: 4.4285106382 \t valid loss: 4.2945563793 \t valid acc: 0.3492\n",
      "Epoch 1565 \t train loss: 4.4403432968 \t valid loss: 4.2844241261 \t valid acc: 0.3531\n",
      "Epoch 1570 \t train loss: 4.4191824558 \t valid loss: 4.2850334048 \t valid acc: 0.3531\n",
      "Epoch 1575 \t train loss: 4.4106597789 \t valid loss: 4.2897628546 \t valid acc: 0.3531\n",
      "Epoch 1580 \t train loss: 4.4478222437 \t valid loss: 4.2898166180 \t valid acc: 0.3492\n",
      "Epoch 1585 \t train loss: 4.4097859305 \t valid loss: 4.2879899144 \t valid acc: 0.3500\n",
      "Epoch 1590 \t train loss: 4.4508375955 \t valid loss: 4.2904155850 \t valid acc: 0.3508\n",
      "Epoch 1595 \t train loss: 4.4644850243 \t valid loss: 4.2861827612 \t valid acc: 0.3492\n",
      "Epoch 1600 \t train loss: 4.4433540633 \t valid loss: 4.2907429338 \t valid acc: 0.3508\n",
      "Epoch 1605 \t train loss: 4.4497193902 \t valid loss: 4.2883812189 \t valid acc: 0.3500\n",
      "Epoch 1610 \t train loss: 4.4174851373 \t valid loss: 4.2866829634 \t valid acc: 0.3500\n",
      "Epoch 1615 \t train loss: 4.4123270456 \t valid loss: 4.2921492457 \t valid acc: 0.3500\n",
      "Epoch 1620 \t train loss: 4.4268539728 \t valid loss: 4.2843768597 \t valid acc: 0.3492\n",
      "Epoch 1625 \t train loss: 4.4795751128 \t valid loss: 4.2851765156 \t valid acc: 0.3508\n",
      "Epoch 1630 \t train loss: 4.4018759118 \t valid loss: 4.2852154374 \t valid acc: 0.3523\n",
      "Epoch 1635 \t train loss: 4.4472107665 \t valid loss: 4.2886046767 \t valid acc: 0.3508\n",
      "Epoch 1640 \t train loss: 4.3802616929 \t valid loss: 4.2878442407 \t valid acc: 0.3523\n",
      "Epoch 1645 \t train loss: 4.3838875294 \t valid loss: 4.2861410379 \t valid acc: 0.3500\n",
      "Epoch 1650 \t train loss: 4.4174027166 \t valid loss: 4.2872456908 \t valid acc: 0.3523\n",
      "Epoch 1655 \t train loss: 4.3979332281 \t valid loss: 4.2827425599 \t valid acc: 0.3500\n",
      "Epoch 1660 \t train loss: 4.4288313001 \t valid loss: 4.2811738849 \t valid acc: 0.3516\n",
      "Epoch 1665 \t train loss: 4.4133459080 \t valid loss: 4.2822630405 \t valid acc: 0.3516\n",
      "Epoch 1670 \t train loss: 4.3881653741 \t valid loss: 4.2845626473 \t valid acc: 0.3484\n",
      "Epoch 1675 \t train loss: 4.4115687137 \t valid loss: 4.2827680111 \t valid acc: 0.3508\n",
      "Epoch 1680 \t train loss: 4.3728853991 \t valid loss: 4.2832850814 \t valid acc: 0.3523\n",
      "Epoch 1685 \t train loss: 4.4181055912 \t valid loss: 4.2832797766 \t valid acc: 0.3523\n",
      "Epoch 1690 \t train loss: 4.4163148403 \t valid loss: 4.2804482579 \t valid acc: 0.3508\n",
      "Epoch 1695 \t train loss: 4.3839353850 \t valid loss: 4.2784993052 \t valid acc: 0.3484\n",
      "Epoch 1700 \t train loss: 4.4197210656 \t valid loss: 4.2836718559 \t valid acc: 0.3492\n",
      "Epoch 1705 \t train loss: 4.4427854738 \t valid loss: 4.2755510211 \t valid acc: 0.3492\n",
      "Epoch 1710 \t train loss: 4.4043636156 \t valid loss: 4.2739174962 \t valid acc: 0.3477\n",
      "Epoch 1715 \t train loss: 4.3896895287 \t valid loss: 4.2768029571 \t valid acc: 0.3492\n",
      "Epoch 1720 \t train loss: 4.3517519541 \t valid loss: 4.2749425769 \t valid acc: 0.3477\n",
      "Epoch 1725 \t train loss: 4.3736392731 \t valid loss: 4.2789301276 \t valid acc: 0.3492\n",
      "Epoch 1730 \t train loss: 4.3615780043 \t valid loss: 4.2751977444 \t valid acc: 0.3484\n",
      "Epoch 1735 \t train loss: 4.3993512032 \t valid loss: 4.2691246271 \t valid acc: 0.3508\n",
      "Epoch 1740 \t train loss: 4.4081837077 \t valid loss: 4.2708406448 \t valid acc: 0.3508\n",
      "Epoch 1745 \t train loss: 4.4317657615 \t valid loss: 4.2767742276 \t valid acc: 0.3492\n",
      "Epoch 1750 \t train loss: 4.3627386814 \t valid loss: 4.2693706751 \t valid acc: 0.3508\n",
      "Epoch 1755 \t train loss: 4.3936726437 \t valid loss: 4.2743146420 \t valid acc: 0.3523\n",
      "Epoch 1760 \t train loss: 4.4207285781 \t valid loss: 4.2736151218 \t valid acc: 0.3516\n",
      "Epoch 1765 \t train loss: 4.3949655655 \t valid loss: 4.2741957903 \t valid acc: 0.3555\n",
      "Epoch 1770 \t train loss: 4.3738534173 \t valid loss: 4.2790130377 \t valid acc: 0.3547\n",
      "Epoch 1775 \t train loss: 4.4190064752 \t valid loss: 4.2732698917 \t valid acc: 0.3492\n",
      "Epoch 1780 \t train loss: 4.4068876311 \t valid loss: 4.2765002251 \t valid acc: 0.3484\n",
      "Epoch 1785 \t train loss: 4.3568902459 \t valid loss: 4.2836553454 \t valid acc: 0.3516\n",
      "Epoch 1790 \t train loss: 4.4081459267 \t valid loss: 4.2769340873 \t valid acc: 0.3516\n",
      "Epoch 1795 \t train loss: 4.4371855758 \t valid loss: 4.2773590088 \t valid acc: 0.3484\n",
      "Epoch 1800 \t train loss: 4.4101609906 \t valid loss: 4.2785174251 \t valid acc: 0.3508\n",
      "Epoch 1805 \t train loss: 4.4019204739 \t valid loss: 4.2759364247 \t valid acc: 0.3508\n",
      "Epoch 1810 \t train loss: 4.3962860606 \t valid loss: 4.2766897082 \t valid acc: 0.3484\n",
      "Epoch 1815 \t train loss: 4.3623459949 \t valid loss: 4.2734365463 \t valid acc: 0.3516\n",
      "Epoch 1820 \t train loss: 4.3619600285 \t valid loss: 4.2746350765 \t valid acc: 0.3492\n",
      "Epoch 1825 \t train loss: 4.3974189925 \t valid loss: 4.2725530267 \t valid acc: 0.3484\n",
      "Epoch 1830 \t train loss: 4.3294312899 \t valid loss: 4.2690451145 \t valid acc: 0.3500\n",
      "Epoch 1835 \t train loss: 4.3299422264 \t valid loss: 4.2741704583 \t valid acc: 0.3469\n",
      "Epoch 1840 \t train loss: 4.3611493942 \t valid loss: 4.2679536939 \t valid acc: 0.3484\n",
      "Epoch 1845 \t train loss: 4.3762313931 \t valid loss: 4.2720853686 \t valid acc: 0.3508\n",
      "Epoch 1850 \t train loss: 4.4056027268 \t valid loss: 4.2715261579 \t valid acc: 0.3500\n",
      "Epoch 1855 \t train loss: 4.3817570653 \t valid loss: 4.2675104141 \t valid acc: 0.3477\n",
      "Epoch 1860 \t train loss: 4.3531624106 \t valid loss: 4.2765083313 \t valid acc: 0.3414\n",
      "Epoch 1865 \t train loss: 4.3702843466 \t valid loss: 4.2673397660 \t valid acc: 0.3484\n",
      "Epoch 1870 \t train loss: 4.4101490642 \t valid loss: 4.2669814825 \t valid acc: 0.3438\n",
      "Epoch 1875 \t train loss: 4.3627683451 \t valid loss: 4.2740795016 \t valid acc: 0.3469\n",
      "Epoch 1880 \t train loss: 4.4053777872 \t valid loss: 4.2680287957 \t valid acc: 0.3469\n",
      "Epoch 1885 \t train loss: 4.3437825358 \t valid loss: 4.2651899457 \t valid acc: 0.3469\n",
      "Epoch 1890 \t train loss: 4.3485528757 \t valid loss: 4.2731108665 \t valid acc: 0.3453\n",
      "Epoch 1895 \t train loss: 4.3669809574 \t valid loss: 4.2641849518 \t valid acc: 0.3445\n",
      "Epoch 1900 \t train loss: 4.3231128926 \t valid loss: 4.2689490914 \t valid acc: 0.3461\n",
      "Epoch 1905 \t train loss: 4.3917923916 \t valid loss: 4.2653999329 \t valid acc: 0.3484\n",
      "Epoch 1910 \t train loss: 4.3735396418 \t valid loss: 4.2625815272 \t valid acc: 0.3500\n",
      "Epoch 1915 \t train loss: 4.3725821528 \t valid loss: 4.2672969103 \t valid acc: 0.3469\n",
      "Epoch 1920 \t train loss: 4.4095812842 \t valid loss: 4.2630335093 \t valid acc: 0.3484\n",
      "Epoch 1925 \t train loss: 4.3714054851 \t valid loss: 4.2659641504 \t valid acc: 0.3477\n",
      "Epoch 1930 \t train loss: 4.3418568511 \t valid loss: 4.2645103931 \t valid acc: 0.3461\n",
      "Epoch 1935 \t train loss: 4.3856676068 \t valid loss: 4.2685939670 \t valid acc: 0.3477\n",
      "Epoch 1940 \t train loss: 4.3734284778 \t valid loss: 4.2613967061 \t valid acc: 0.3469\n",
      "Epoch 1945 \t train loss: 4.3654989974 \t valid loss: 4.2679092884 \t valid acc: 0.3477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1950 \t train loss: 4.3719546074 \t valid loss: 4.2691670060 \t valid acc: 0.3453\n",
      "Epoch 1955 \t train loss: 4.3698636709 \t valid loss: 4.2651998997 \t valid acc: 0.3484\n",
      "Epoch 1960 \t train loss: 4.3294925856 \t valid loss: 4.2659943700 \t valid acc: 0.3438\n",
      "Epoch 1965 \t train loss: 4.3962045270 \t valid loss: 4.2636640072 \t valid acc: 0.3445\n",
      "Epoch 1970 \t train loss: 4.3913139133 \t valid loss: 4.2601923347 \t valid acc: 0.3500\n",
      "Epoch 1975 \t train loss: 4.3457405290 \t valid loss: 4.2631229162 \t valid acc: 0.3461\n",
      "Epoch 1980 \t train loss: 4.3632475221 \t valid loss: 4.2729855776 \t valid acc: 0.3461\n",
      "Epoch 1985 \t train loss: 4.3494820207 \t valid loss: 4.2604335546 \t valid acc: 0.3461\n",
      "Epoch 1990 \t train loss: 4.3652750503 \t valid loss: 4.2632695436 \t valid acc: 0.3453\n",
      "Epoch 1995 \t train loss: 4.3183308202 \t valid loss: 4.2679557204 \t valid acc: 0.3477\n",
      "Epoch 2000 \t train loss: 4.3421893896 \t valid loss: 4.2720269561 \t valid acc: 0.3492\n",
      "Epoch 2005 \t train loss: 4.3145859574 \t valid loss: 4.2642872930 \t valid acc: 0.3477\n",
      "Epoch 2010 \t train loss: 4.3255276625 \t valid loss: 4.2677605748 \t valid acc: 0.3445\n",
      "Epoch 2015 \t train loss: 4.3910850314 \t valid loss: 4.2672950625 \t valid acc: 0.3469\n",
      "Epoch 2020 \t train loss: 4.3372477376 \t valid loss: 4.2710407376 \t valid acc: 0.3438\n",
      "Epoch 2025 \t train loss: 4.3596582524 \t valid loss: 4.2614708543 \t valid acc: 0.3484\n",
      "Epoch 2030 \t train loss: 4.3443923884 \t valid loss: 4.2567630410 \t valid acc: 0.3461\n",
      "Epoch 2035 \t train loss: 4.3445302276 \t valid loss: 4.2670737505 \t valid acc: 0.3422\n",
      "Epoch 2040 \t train loss: 4.3309255811 \t valid loss: 4.2605969906 \t valid acc: 0.3414\n",
      "Epoch 2045 \t train loss: 4.4026267362 \t valid loss: 4.2624911666 \t valid acc: 0.3406\n",
      "Epoch 2050 \t train loss: 4.3460441578 \t valid loss: 4.2659963369 \t valid acc: 0.3406\n",
      "Epoch 2055 \t train loss: 4.3543790385 \t valid loss: 4.2640231252 \t valid acc: 0.3430\n",
      "Epoch 2060 \t train loss: 4.3457740185 \t valid loss: 4.2673234344 \t valid acc: 0.3430\n",
      "Epoch 2065 \t train loss: 4.3199567573 \t valid loss: 4.2620848417 \t valid acc: 0.3430\n",
      "Epoch 2070 \t train loss: 4.3648246721 \t valid loss: 4.2640223503 \t valid acc: 0.3430\n",
      "Epoch 2075 \t train loss: 4.3530665886 \t valid loss: 4.2572131753 \t valid acc: 0.3414\n",
      "Epoch 2080 \t train loss: 4.3178135628 \t valid loss: 4.2628693581 \t valid acc: 0.3422\n",
      "Epoch 2085 \t train loss: 4.3319837714 \t valid loss: 4.2593881488 \t valid acc: 0.3430\n",
      "Epoch 2090 \t train loss: 4.3649916039 \t valid loss: 4.2583238482 \t valid acc: 0.3438\n",
      "Epoch 2095 \t train loss: 4.3594274687 \t valid loss: 4.2607867718 \t valid acc: 0.3414\n",
      "Epoch 2100 \t train loss: 4.3573525063 \t valid loss: 4.2647839189 \t valid acc: 0.3484\n",
      "Epoch 2105 \t train loss: 4.3088910247 \t valid loss: 4.2642804980 \t valid acc: 0.3414\n",
      "Epoch 2110 \t train loss: 4.2951461770 \t valid loss: 4.2616806030 \t valid acc: 0.3430\n",
      "Epoch 2115 \t train loss: 4.3625225245 \t valid loss: 4.2643696666 \t valid acc: 0.3414\n",
      "Epoch 2120 \t train loss: 4.3414574667 \t valid loss: 4.2599363327 \t valid acc: 0.3469\n",
      "Epoch 2125 \t train loss: 4.3530299719 \t valid loss: 4.2583510280 \t valid acc: 0.3430\n",
      "Epoch 2130 \t train loss: 4.3318096372 \t valid loss: 4.2632394433 \t valid acc: 0.3398\n",
      "Epoch 2135 \t train loss: 4.3234597084 \t valid loss: 4.2574196458 \t valid acc: 0.3422\n",
      "Epoch 2140 \t train loss: 4.3167700934 \t valid loss: 4.2577522397 \t valid acc: 0.3422\n",
      "Epoch 2145 \t train loss: 4.3531067427 \t valid loss: 4.2564011216 \t valid acc: 0.3453\n",
      "Epoch 2150 \t train loss: 4.2848020986 \t valid loss: 4.2553389072 \t valid acc: 0.3445\n",
      "Epoch 2155 \t train loss: 4.3317642877 \t valid loss: 4.2590605617 \t valid acc: 0.3406\n",
      "Epoch 2160 \t train loss: 4.3363498200 \t valid loss: 4.2532759905 \t valid acc: 0.3422\n",
      "Epoch 2165 \t train loss: 4.3558504970 \t valid loss: 4.2587066889 \t valid acc: 0.3391\n",
      "Epoch 2170 \t train loss: 4.3349838479 \t valid loss: 4.2627864480 \t valid acc: 0.3438\n",
      "Epoch 2175 \t train loss: 4.3106932640 \t valid loss: 4.2611753345 \t valid acc: 0.3414\n",
      "Epoch 2180 \t train loss: 4.3004576772 \t valid loss: 4.2599482536 \t valid acc: 0.3414\n",
      "Epoch 2185 \t train loss: 4.3290877509 \t valid loss: 4.2506641150 \t valid acc: 0.3445\n",
      "Epoch 2190 \t train loss: 4.3331875746 \t valid loss: 4.2593528032 \t valid acc: 0.3461\n",
      "Epoch 2195 \t train loss: 4.3311898154 \t valid loss: 4.2596116662 \t valid acc: 0.3438\n",
      "Epoch 2200 \t train loss: 4.3000835873 \t valid loss: 4.2576548457 \t valid acc: 0.3422\n",
      "Epoch 2205 \t train loss: 4.3626792098 \t valid loss: 4.2605029941 \t valid acc: 0.3414\n",
      "Epoch 2210 \t train loss: 4.3053759808 \t valid loss: 4.2608465552 \t valid acc: 0.3414\n",
      "Epoch 2215 \t train loss: 4.2755930812 \t valid loss: 4.2588469386 \t valid acc: 0.3414\n",
      "Epoch 2220 \t train loss: 4.3004423352 \t valid loss: 4.2616111636 \t valid acc: 0.3438\n",
      "Epoch 2225 \t train loss: 4.2631637607 \t valid loss: 4.2628850341 \t valid acc: 0.3430\n",
      "Epoch 2230 \t train loss: 4.2891368478 \t valid loss: 4.2599746585 \t valid acc: 0.3422\n",
      "Epoch 2235 \t train loss: 4.3224528778 \t valid loss: 4.2598600388 \t valid acc: 0.3430\n",
      "Epoch 2240 \t train loss: 4.3453183008 \t valid loss: 4.2576294541 \t valid acc: 0.3406\n",
      "Epoch 2245 \t train loss: 4.3037580169 \t valid loss: 4.2588042617 \t valid acc: 0.3445\n",
      "Epoch 2250 \t train loss: 4.2823394010 \t valid loss: 4.2543405294 \t valid acc: 0.3430\n",
      "Epoch 2255 \t train loss: 4.2870930905 \t valid loss: 4.2586091161 \t valid acc: 0.3398\n",
      "Epoch 2260 \t train loss: 4.2965294760 \t valid loss: 4.2505244613 \t valid acc: 0.3445\n",
      "Epoch 2265 \t train loss: 4.2951455837 \t valid loss: 4.2508081198 \t valid acc: 0.3391\n",
      "Epoch 2270 \t train loss: 4.3199298992 \t valid loss: 4.2530922294 \t valid acc: 0.3383\n",
      "Epoch 2275 \t train loss: 4.3213821067 \t valid loss: 4.2517071962 \t valid acc: 0.3453\n",
      "Epoch 2280 \t train loss: 4.3111408478 \t valid loss: 4.2564190626 \t valid acc: 0.3383\n",
      "Epoch 2285 \t train loss: 4.3064825036 \t valid loss: 4.2551122904 \t valid acc: 0.3438\n",
      "Epoch 2290 \t train loss: 4.3351104315 \t valid loss: 4.2583335042 \t valid acc: 0.3438\n",
      "Epoch 2295 \t train loss: 4.3412130988 \t valid loss: 4.2571971416 \t valid acc: 0.3430\n",
      "Epoch 2300 \t train loss: 4.2753873426 \t valid loss: 4.2508119941 \t valid acc: 0.3414\n",
      "Epoch 2305 \t train loss: 4.2868711893 \t valid loss: 4.2557131648 \t valid acc: 0.3398\n",
      "Epoch 2310 \t train loss: 4.2950660961 \t valid loss: 4.2569561601 \t valid acc: 0.3438\n",
      "Epoch 2315 \t train loss: 4.3029484527 \t valid loss: 4.2554543018 \t valid acc: 0.3422\n",
      "Epoch 2320 \t train loss: 4.3076505328 \t valid loss: 4.2541737556 \t valid acc: 0.3430\n",
      "Epoch 2325 \t train loss: 4.2790865676 \t valid loss: 4.2584554553 \t valid acc: 0.3422\n",
      "Epoch 2330 \t train loss: 4.3071308580 \t valid loss: 4.2547152042 \t valid acc: 0.3438\n",
      "Epoch 2335 \t train loss: 4.3260689447 \t valid loss: 4.2631614208 \t valid acc: 0.3422\n",
      "Epoch 2340 \t train loss: 4.2743937415 \t valid loss: 4.2537885308 \t valid acc: 0.3414\n",
      "Epoch 2345 \t train loss: 4.2952285090 \t valid loss: 4.2557800412 \t valid acc: 0.3445\n",
      "Epoch 2350 \t train loss: 4.2688785265 \t valid loss: 4.2518951893 \t valid acc: 0.3414\n",
      "Epoch 2355 \t train loss: 4.2814034917 \t valid loss: 4.2540835142 \t valid acc: 0.3422\n",
      "Epoch 2360 \t train loss: 4.3106536699 \t valid loss: 4.2521146536 \t valid acc: 0.3453\n",
      "Epoch 2365 \t train loss: 4.2564808768 \t valid loss: 4.2496981621 \t valid acc: 0.3445\n",
      "Epoch 2370 \t train loss: 4.2940020229 \t valid loss: 4.2501395941 \t valid acc: 0.3422\n",
      "Epoch 2375 \t train loss: 4.2739290858 \t valid loss: 4.2505891323 \t valid acc: 0.3438\n",
      "Epoch 2380 \t train loss: 4.2844109979 \t valid loss: 4.2551476359 \t valid acc: 0.3453\n",
      "Epoch 2385 \t train loss: 4.3077167411 \t valid loss: 4.2515037060 \t valid acc: 0.3445\n",
      "Epoch 2390 \t train loss: 4.2769006019 \t valid loss: 4.2476690412 \t valid acc: 0.3453\n",
      "Epoch 2395 \t train loss: 4.2388267961 \t valid loss: 4.2639774680 \t valid acc: 0.3383\n",
      "Epoch 2400 \t train loss: 4.3158496701 \t valid loss: 4.2614395618 \t valid acc: 0.3367\n",
      "Epoch 2405 \t train loss: 4.2867954831 \t valid loss: 4.2547760606 \t valid acc: 0.3406\n",
      "Epoch 2410 \t train loss: 4.2399253901 \t valid loss: 4.2478628755 \t valid acc: 0.3453\n",
      "Epoch 2415 \t train loss: 4.2967714321 \t valid loss: 4.2501597404 \t valid acc: 0.3453\n",
      "Epoch 2420 \t train loss: 4.3308155537 \t valid loss: 4.2558949590 \t valid acc: 0.3406\n",
      "Epoch 2425 \t train loss: 4.2868627670 \t valid loss: 4.2578025460 \t valid acc: 0.3414\n",
      "Epoch 2430 \t train loss: 4.2853151698 \t valid loss: 4.2530739307 \t valid acc: 0.3445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2435 \t train loss: 4.2860363805 \t valid loss: 4.2439427972 \t valid acc: 0.3469\n",
      "Epoch 2440 \t train loss: 4.3098401580 \t valid loss: 4.2567154765 \t valid acc: 0.3430\n",
      "Epoch 2445 \t train loss: 4.3109145664 \t valid loss: 4.2497533560 \t valid acc: 0.3445\n",
      "Epoch 2450 \t train loss: 4.2856134038 \t valid loss: 4.2472531199 \t valid acc: 0.3430\n",
      "Epoch 2455 \t train loss: 4.2286542626 \t valid loss: 4.2502418160 \t valid acc: 0.3430\n",
      "Epoch 2460 \t train loss: 4.3073774548 \t valid loss: 4.2646985054 \t valid acc: 0.3383\n",
      "Epoch 2465 \t train loss: 4.2787637433 \t valid loss: 4.2486873865 \t valid acc: 0.3430\n",
      "Epoch 2470 \t train loss: 4.2592806040 \t valid loss: 4.2432086468 \t valid acc: 0.3422\n",
      "Epoch 2475 \t train loss: 4.2534875870 \t valid loss: 4.2406216860 \t valid acc: 0.3445\n",
      "Epoch 2480 \t train loss: 4.2922847825 \t valid loss: 4.2416529655 \t valid acc: 0.3461\n",
      "Epoch 2485 \t train loss: 4.2635853457 \t valid loss: 4.2375959158 \t valid acc: 0.3469\n",
      "Epoch 2490 \t train loss: 4.2622098923 \t valid loss: 4.2453745008 \t valid acc: 0.3484\n",
      "Epoch 2495 \t train loss: 4.2231348004 \t valid loss: 4.2407374382 \t valid acc: 0.3469\n",
      "Epoch 2500 \t train loss: 4.2735540257 \t valid loss: 4.2501865029 \t valid acc: 0.3469\n",
      "Epoch 2505 \t train loss: 4.2462712554 \t valid loss: 4.2457228899 \t valid acc: 0.3500\n",
      "Epoch 2510 \t train loss: 4.2684686794 \t valid loss: 4.2399135828 \t valid acc: 0.3484\n",
      "Epoch 2515 \t train loss: 4.2834100224 \t valid loss: 4.2416996360 \t valid acc: 0.3438\n",
      "Epoch 2520 \t train loss: 4.2867130568 \t valid loss: 4.2375580072 \t valid acc: 0.3445\n",
      "Epoch 2525 \t train loss: 4.2564464724 \t valid loss: 4.2383353114 \t valid acc: 0.3422\n",
      "Epoch 2530 \t train loss: 4.2867710313 \t valid loss: 4.2447742224 \t valid acc: 0.3453\n",
      "Epoch 2535 \t train loss: 4.2377367685 \t valid loss: 4.2483329177 \t valid acc: 0.3445\n",
      "Epoch 2540 \t train loss: 4.2634214745 \t valid loss: 4.2471902370 \t valid acc: 0.3453\n",
      "Epoch 2545 \t train loss: 4.2592067275 \t valid loss: 4.2471412420 \t valid acc: 0.3445\n",
      "Epoch 2550 \t train loss: 4.2391818679 \t valid loss: 4.2531011701 \t valid acc: 0.3445\n",
      "Epoch 2555 \t train loss: 4.2496886198 \t valid loss: 4.2574878931 \t valid acc: 0.3406\n",
      "Epoch 2560 \t train loss: 4.2701223340 \t valid loss: 4.2413022518 \t valid acc: 0.3469\n",
      "Epoch 2565 \t train loss: 4.2629469883 \t valid loss: 4.2499596477 \t valid acc: 0.3406\n",
      "Epoch 2570 \t train loss: 4.2209262848 \t valid loss: 4.2442860007 \t valid acc: 0.3398\n",
      "Epoch 2575 \t train loss: 4.2915046104 \t valid loss: 4.2535427213 \t valid acc: 0.3414\n",
      "Epoch 2580 \t train loss: 4.2461440785 \t valid loss: 4.2388659120 \t valid acc: 0.3438\n",
      "Epoch 2585 \t train loss: 4.2594342232 \t valid loss: 4.2442170978 \t valid acc: 0.3406\n",
      "Epoch 2590 \t train loss: 4.2439836291 \t valid loss: 4.2524182200 \t valid acc: 0.3375\n",
      "Epoch 2595 \t train loss: 4.2658872937 \t valid loss: 4.2405407429 \t valid acc: 0.3422\n",
      "Epoch 2600 \t train loss: 4.2384873490 \t valid loss: 4.2369050980 \t valid acc: 0.3461\n",
      "Epoch 2605 \t train loss: 4.2529720317 \t valid loss: 4.2354350686 \t valid acc: 0.3453\n",
      "Epoch 2610 \t train loss: 4.2368195057 \t valid loss: 4.2441977262 \t valid acc: 0.3461\n",
      "Epoch 2615 \t train loss: 4.1910973760 \t valid loss: 4.2343382835 \t valid acc: 0.3469\n",
      "Epoch 2620 \t train loss: 4.2478629101 \t valid loss: 4.2392699122 \t valid acc: 0.3469\n",
      "Epoch 2625 \t train loss: 4.2957314447 \t valid loss: 4.2443201542 \t valid acc: 0.3406\n",
      "Epoch 2630 \t train loss: 4.2434038617 \t valid loss: 4.2343330383 \t valid acc: 0.3469\n",
      "Epoch 2635 \t train loss: 4.2886205718 \t valid loss: 4.2311911583 \t valid acc: 0.3453\n",
      "Epoch 2640 \t train loss: 4.2173278276 \t valid loss: 4.2353482842 \t valid acc: 0.3445\n",
      "Epoch 2645 \t train loss: 4.2346127477 \t valid loss: 4.2328890562 \t valid acc: 0.3430\n",
      "Epoch 2650 \t train loss: 4.1890923256 \t valid loss: 4.2290285230 \t valid acc: 0.3469\n",
      "Epoch 2655 \t train loss: 4.2133234767 \t valid loss: 4.2336537838 \t valid acc: 0.3445\n",
      "Epoch 2660 \t train loss: 4.2373770337 \t valid loss: 4.2389925718 \t valid acc: 0.3430\n",
      "Epoch 2665 \t train loss: 4.2499034571 \t valid loss: 4.2360701561 \t valid acc: 0.3477\n",
      "Epoch 2670 \t train loss: 4.2349850632 \t valid loss: 4.2421656847 \t valid acc: 0.3461\n",
      "Epoch 2675 \t train loss: 4.1830742193 \t valid loss: 4.2330002785 \t valid acc: 0.3438\n",
      "Epoch 2680 \t train loss: 4.1768459442 \t valid loss: 4.2347936034 \t valid acc: 0.3461\n",
      "Epoch 2685 \t train loss: 4.1958376973 \t valid loss: 4.2358714938 \t valid acc: 0.3430\n",
      "Epoch 2690 \t train loss: 4.2747625417 \t valid loss: 4.2318869829 \t valid acc: 0.3469\n",
      "Epoch 2695 \t train loss: 4.2438854783 \t valid loss: 4.2302877903 \t valid acc: 0.3461\n",
      "Epoch 2700 \t train loss: 4.2500400820 \t valid loss: 4.2383074164 \t valid acc: 0.3422\n",
      "Epoch 2705 \t train loss: 4.2669079193 \t valid loss: 4.2174140811 \t valid acc: 0.3453\n",
      "Epoch 2710 \t train loss: 4.2187793255 \t valid loss: 4.2168674469 \t valid acc: 0.3461\n",
      "Epoch 2715 \t train loss: 4.2587883805 \t valid loss: 4.2416709065 \t valid acc: 0.3430\n",
      "Epoch 2720 \t train loss: 4.2065405236 \t valid loss: 4.2325838208 \t valid acc: 0.3469\n",
      "Epoch 2725 \t train loss: 4.2640228771 \t valid loss: 4.2299858928 \t valid acc: 0.3492\n",
      "Epoch 2730 \t train loss: 4.2283323366 \t valid loss: 4.2157689929 \t valid acc: 0.3477\n",
      "Epoch 2735 \t train loss: 4.2357097559 \t valid loss: 4.2237284780 \t valid acc: 0.3469\n",
      "Epoch 2740 \t train loss: 4.2499082310 \t valid loss: 4.2215909362 \t valid acc: 0.3469\n",
      "Epoch 2745 \t train loss: 4.2433113497 \t valid loss: 4.2155724764 \t valid acc: 0.3477\n",
      "Epoch 2750 \t train loss: 4.2528412841 \t valid loss: 4.2259770036 \t valid acc: 0.3461\n",
      "Epoch 2755 \t train loss: 4.2148788197 \t valid loss: 4.2352702618 \t valid acc: 0.3430\n",
      "Epoch 2760 \t train loss: 4.1614261838 \t valid loss: 4.2423439622 \t valid acc: 0.3445\n",
      "Epoch 2765 \t train loss: 4.2340809634 \t valid loss: 4.2275109887 \t valid acc: 0.3414\n",
      "Epoch 2770 \t train loss: 4.1943495163 \t valid loss: 4.2303479910 \t valid acc: 0.3422\n",
      "Epoch 2775 \t train loss: 4.2060595945 \t valid loss: 4.2298009396 \t valid acc: 0.3445\n",
      "Epoch 2780 \t train loss: 4.2227619692 \t valid loss: 4.2309019566 \t valid acc: 0.3453\n",
      "Epoch 2785 \t train loss: 4.2286748997 \t valid loss: 4.2376431823 \t valid acc: 0.3445\n",
      "Epoch 2790 \t train loss: 4.1913878086 \t valid loss: 4.2307860851 \t valid acc: 0.3461\n",
      "Epoch 2795 \t train loss: 4.2209674924 \t valid loss: 4.2311565876 \t valid acc: 0.3469\n",
      "Epoch 2800 \t train loss: 4.2158564135 \t valid loss: 4.2274190187 \t valid acc: 0.3500\n",
      "Epoch 2805 \t train loss: 4.2457823310 \t valid loss: 4.2285380960 \t valid acc: 0.3461\n",
      "Epoch 2810 \t train loss: 4.2326951803 \t valid loss: 4.2249328494 \t valid acc: 0.3453\n",
      "Epoch 2815 \t train loss: 4.2345087029 \t valid loss: 4.2290206552 \t valid acc: 0.3461\n",
      "Epoch 2820 \t train loss: 4.2180663763 \t valid loss: 4.2234284282 \t valid acc: 0.3484\n",
      "Epoch 2825 \t train loss: 4.2056798769 \t valid loss: 4.2316545248 \t valid acc: 0.3430\n",
      "Epoch 2830 \t train loss: 4.2258213675 \t valid loss: 4.2168223262 \t valid acc: 0.3484\n",
      "Epoch 2835 \t train loss: 4.2033552735 \t valid loss: 4.2162805796 \t valid acc: 0.3477\n",
      "Epoch 2840 \t train loss: 4.1858431129 \t valid loss: 4.2248470783 \t valid acc: 0.3453\n",
      "Epoch 2845 \t train loss: 4.2233452298 \t valid loss: 4.2231324911 \t valid acc: 0.3445\n",
      "Epoch 2850 \t train loss: 4.2587620491 \t valid loss: 4.2321886420 \t valid acc: 0.3445\n",
      "Epoch 2855 \t train loss: 4.2091633386 \t valid loss: 4.2318202257 \t valid acc: 0.3422\n",
      "Epoch 2860 \t train loss: 4.1800598156 \t valid loss: 4.2317386866 \t valid acc: 0.3445\n",
      "Epoch 2865 \t train loss: 4.1673732081 \t valid loss: 4.2249802351 \t valid acc: 0.3492\n",
      "Epoch 2870 \t train loss: 4.2209936464 \t valid loss: 4.2302901745 \t valid acc: 0.3484\n",
      "Epoch 2875 \t train loss: 4.1826770472 \t valid loss: 4.2341858745 \t valid acc: 0.3469\n",
      "Epoch 2880 \t train loss: 4.1674602808 \t valid loss: 4.2239342332 \t valid acc: 0.3422\n",
      "Epoch 2885 \t train loss: 4.1840309431 \t valid loss: 4.2301716208 \t valid acc: 0.3469\n",
      "Epoch 2890 \t train loss: 4.1999718699 \t valid loss: 4.2302266955 \t valid acc: 0.3453\n",
      "Epoch 2895 \t train loss: 4.1688817213 \t valid loss: 4.2290514112 \t valid acc: 0.3430\n",
      "Epoch 2900 \t train loss: 4.2090046018 \t valid loss: 4.2200734019 \t valid acc: 0.3453\n",
      "Epoch 2905 \t train loss: 4.2104379188 \t valid loss: 4.2165135741 \t valid acc: 0.3477\n",
      "Epoch 2910 \t train loss: 4.2069914785 \t valid loss: 4.2205476165 \t valid acc: 0.3477\n",
      "Epoch 2915 \t train loss: 4.1723576756 \t valid loss: 4.2188134789 \t valid acc: 0.3469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2920 \t train loss: 4.1887434638 \t valid loss: 4.2155413032 \t valid acc: 0.3430\n",
      "Epoch 2925 \t train loss: 4.2044234664 \t valid loss: 4.2205806971 \t valid acc: 0.3453\n",
      "Epoch 2930 \t train loss: 4.1625230700 \t valid loss: 4.2124108076 \t valid acc: 0.3477\n",
      "Epoch 2935 \t train loss: 4.2020769785 \t valid loss: 4.2198703289 \t valid acc: 0.3484\n",
      "Epoch 2940 \t train loss: 4.1928583411 \t valid loss: 4.2252908945 \t valid acc: 0.3461\n",
      "Epoch 2945 \t train loss: 4.1725039316 \t valid loss: 4.2363021970 \t valid acc: 0.3453\n",
      "Epoch 2950 \t train loss: 4.1981837029 \t valid loss: 4.2118260264 \t valid acc: 0.3445\n",
      "Epoch 2955 \t train loss: 4.2163317758 \t valid loss: 4.2204607129 \t valid acc: 0.3461\n",
      "Epoch 2960 \t train loss: 4.1675267109 \t valid loss: 4.2262032628 \t valid acc: 0.3453\n",
      "Epoch 2965 \t train loss: 4.2118188670 \t valid loss: 4.2136043906 \t valid acc: 0.3469\n",
      "Epoch 2970 \t train loss: 4.1769208742 \t valid loss: 4.2176504135 \t valid acc: 0.3445\n",
      "Epoch 2975 \t train loss: 4.1961786414 \t valid loss: 4.2157086134 \t valid acc: 0.3461\n",
      "Epoch 2980 \t train loss: 4.1926327473 \t valid loss: 4.2104938626 \t valid acc: 0.3438\n",
      "Epoch 2985 \t train loss: 4.1982062916 \t valid loss: 4.2241836786 \t valid acc: 0.3477\n",
      "Epoch 2990 \t train loss: 4.2055291131 \t valid loss: 4.2084981799 \t valid acc: 0.3461\n",
      "Epoch 2995 \t train loss: 4.1389079538 \t valid loss: 4.2072431445 \t valid acc: 0.3430\n",
      "Epoch 3000 \t train loss: 4.1880022371 \t valid loss: 4.2107743025 \t valid acc: 0.3422\n",
      "Epoch 3005 \t train loss: 4.1805684400 \t valid loss: 4.2079753876 \t valid acc: 0.3445\n",
      "Epoch 3010 \t train loss: 4.2147434922 \t valid loss: 4.2224186659 \t valid acc: 0.3453\n",
      "Epoch 3015 \t train loss: 4.1957592687 \t valid loss: 4.2154329419 \t valid acc: 0.3430\n",
      "Epoch 3020 \t train loss: 4.1711254952 \t valid loss: 4.2191592455 \t valid acc: 0.3477\n",
      "Epoch 3025 \t train loss: 4.1778528191 \t valid loss: 4.2188395858 \t valid acc: 0.3461\n",
      "Epoch 3030 \t train loss: 4.2025377418 \t valid loss: 4.2017489672 \t valid acc: 0.3461\n",
      "Epoch 3035 \t train loss: 4.1807916275 \t valid loss: 4.2099712491 \t valid acc: 0.3438\n",
      "Epoch 3040 \t train loss: 4.1698389885 \t valid loss: 4.2119989395 \t valid acc: 0.3461\n",
      "Epoch 3045 \t train loss: 4.1303635253 \t valid loss: 4.2062678933 \t valid acc: 0.3406\n",
      "Epoch 3050 \t train loss: 4.1573745816 \t valid loss: 4.2201423049 \t valid acc: 0.3430\n",
      "Epoch 3055 \t train loss: 4.1802117658 \t valid loss: 4.2283936143 \t valid acc: 0.3430\n",
      "Epoch 3060 \t train loss: 4.2061868878 \t valid loss: 4.2069905996 \t valid acc: 0.3406\n",
      "Epoch 3065 \t train loss: 4.2140463618 \t valid loss: 4.2211295366 \t valid acc: 0.3414\n",
      "Epoch 3070 \t train loss: 4.1316012615 \t valid loss: 4.2092366219 \t valid acc: 0.3414\n",
      "Epoch 3075 \t train loss: 4.1462014553 \t valid loss: 4.2134797573 \t valid acc: 0.3422\n",
      "Epoch 3080 \t train loss: 4.1593893794 \t valid loss: 4.2083626986 \t valid acc: 0.3391\n",
      "Epoch 3085 \t train loss: 4.1625224657 \t valid loss: 4.2311635017 \t valid acc: 0.3414\n",
      "Epoch 3090 \t train loss: 4.1836314534 \t valid loss: 4.2232407928 \t valid acc: 0.3414\n",
      "Epoch 3095 \t train loss: 4.1409709620 \t valid loss: 4.2122772336 \t valid acc: 0.3406\n",
      "Epoch 3100 \t train loss: 4.1820521521 \t valid loss: 4.2037112713 \t valid acc: 0.3445\n",
      "Epoch 3105 \t train loss: 4.1583514546 \t valid loss: 4.2032339573 \t valid acc: 0.3453\n",
      "Epoch 3110 \t train loss: 4.1228763614 \t valid loss: 4.2062315345 \t valid acc: 0.3445\n",
      "Epoch 3115 \t train loss: 4.1232714819 \t valid loss: 4.2085937262 \t valid acc: 0.3430\n",
      "Epoch 3120 \t train loss: 4.1359060975 \t valid loss: 4.2061212063 \t valid acc: 0.3453\n",
      "Epoch 3125 \t train loss: 4.1473749571 \t valid loss: 4.2334176898 \t valid acc: 0.3367\n",
      "Epoch 3130 \t train loss: 4.1176129607 \t valid loss: 4.2086558342 \t valid acc: 0.3438\n",
      "Epoch 3135 \t train loss: 4.1466894649 \t valid loss: 4.2246526480 \t valid acc: 0.3367\n",
      "Epoch 3140 \t train loss: 4.1219678923 \t valid loss: 4.2053111196 \t valid acc: 0.3422\n",
      "Epoch 3145 \t train loss: 4.1724011953 \t valid loss: 4.2046416402 \t valid acc: 0.3406\n",
      "Epoch 3150 \t train loss: 4.1537630392 \t valid loss: 4.2013581395 \t valid acc: 0.3352\n",
      "Epoch 3155 \t train loss: 4.1697249246 \t valid loss: 4.2012714744 \t valid acc: 0.3422\n",
      "Epoch 3160 \t train loss: 4.1542738704 \t valid loss: 4.1990679502 \t valid acc: 0.3375\n",
      "Epoch 3165 \t train loss: 4.1438146802 \t valid loss: 4.2176652551 \t valid acc: 0.3398\n",
      "Epoch 3170 \t train loss: 4.1479253159 \t valid loss: 4.2015764117 \t valid acc: 0.3453\n",
      "Epoch 3175 \t train loss: 4.1574021439 \t valid loss: 4.2201287746 \t valid acc: 0.3391\n",
      "Epoch 3180 \t train loss: 4.1132922062 \t valid loss: 4.1939743757 \t valid acc: 0.3445\n",
      "Epoch 3185 \t train loss: 4.1522960552 \t valid loss: 4.2221968174 \t valid acc: 0.3438\n",
      "Epoch 3190 \t train loss: 4.1203098519 \t valid loss: 4.2010812163 \t valid acc: 0.3375\n",
      "Epoch 3195 \t train loss: 4.1307331906 \t valid loss: 4.2132020593 \t valid acc: 0.3344\n",
      "Epoch 3200 \t train loss: 4.1220416302 \t valid loss: 4.2039908767 \t valid acc: 0.3438\n",
      "Epoch 3205 \t train loss: 4.1737408860 \t valid loss: 4.1982057691 \t valid acc: 0.3367\n",
      "Epoch 3210 \t train loss: 4.1579417961 \t valid loss: 4.2073292732 \t valid acc: 0.3414\n",
      "Epoch 3215 \t train loss: 4.1489975785 \t valid loss: 4.1931309700 \t valid acc: 0.3375\n",
      "Epoch 3220 \t train loss: 4.1515120628 \t valid loss: 4.2042725086 \t valid acc: 0.3375\n",
      "Epoch 3225 \t train loss: 4.1212367568 \t valid loss: 4.2169057131 \t valid acc: 0.3375\n",
      "Epoch 3230 \t train loss: 4.1322221257 \t valid loss: 4.2007617354 \t valid acc: 0.3406\n",
      "Epoch 3235 \t train loss: 4.1232318601 \t valid loss: 4.1976550817 \t valid acc: 0.3414\n",
      "Epoch 3240 \t train loss: 4.1592108261 \t valid loss: 4.2166620493 \t valid acc: 0.3367\n",
      "Epoch 3245 \t train loss: 4.1343175755 \t valid loss: 4.2096280456 \t valid acc: 0.3383\n",
      "Epoch 3250 \t train loss: 4.1182810817 \t valid loss: 4.2066675425 \t valid acc: 0.3391\n",
      "Epoch 3255 \t train loss: 4.1388172992 \t valid loss: 4.2035294175 \t valid acc: 0.3359\n",
      "Epoch 3260 \t train loss: 4.1345471948 \t valid loss: 4.2241144776 \t valid acc: 0.3391\n",
      "Epoch 3265 \t train loss: 4.0949337205 \t valid loss: 4.2022500038 \t valid acc: 0.3352\n",
      "Epoch 3270 \t train loss: 4.1295561347 \t valid loss: 4.2106764317 \t valid acc: 0.3344\n",
      "Epoch 3275 \t train loss: 4.0877485719 \t valid loss: 4.2079976201 \t valid acc: 0.3406\n",
      "Epoch 3280 \t train loss: 4.1482281463 \t valid loss: 4.2039069533 \t valid acc: 0.3375\n",
      "Epoch 3285 \t train loss: 4.1415067883 \t valid loss: 4.2256538868 \t valid acc: 0.3313\n",
      "Epoch 3290 \t train loss: 4.1032796128 \t valid loss: 4.2171087861 \t valid acc: 0.3352\n",
      "Epoch 3295 \t train loss: 4.1437137127 \t valid loss: 4.2098438740 \t valid acc: 0.3367\n",
      "Epoch 3300 \t train loss: 4.1163445573 \t valid loss: 4.2179149389 \t valid acc: 0.3320\n",
      "Epoch 3305 \t train loss: 4.1343351464 \t valid loss: 4.2096089721 \t valid acc: 0.3359\n",
      "Epoch 3310 \t train loss: 4.1088444133 \t valid loss: 4.2237188816 \t valid acc: 0.3344\n",
      "Epoch 3315 \t train loss: 4.1846569416 \t valid loss: 4.2049582005 \t valid acc: 0.3328\n",
      "Epoch 3320 \t train loss: 4.1421053132 \t valid loss: 4.2074948549 \t valid acc: 0.3367\n",
      "Epoch 3325 \t train loss: 4.1220663703 \t valid loss: 4.2203616500 \t valid acc: 0.3391\n",
      "Epoch 3330 \t train loss: 4.1000521516 \t valid loss: 4.2279605865 \t valid acc: 0.3344\n",
      "Epoch 3335 \t train loss: 4.1170464172 \t valid loss: 4.1998960972 \t valid acc: 0.3359\n",
      "Epoch 3340 \t train loss: 4.1008587937 \t valid loss: 4.2006424069 \t valid acc: 0.3344\n",
      "Epoch 3345 \t train loss: 4.0883845950 \t valid loss: 4.2093700171 \t valid acc: 0.3352\n",
      "Epoch 3350 \t train loss: 4.1536212522 \t valid loss: 4.1987090111 \t valid acc: 0.3367\n",
      "Epoch 3355 \t train loss: 4.1197217731 \t valid loss: 4.1982752085 \t valid acc: 0.3352\n",
      "Epoch 3360 \t train loss: 4.0992275227 \t valid loss: 4.1974702477 \t valid acc: 0.3352\n",
      "Epoch 3365 \t train loss: 4.1228916257 \t valid loss: 4.2132396698 \t valid acc: 0.3359\n",
      "Epoch 3370 \t train loss: 4.1224279792 \t valid loss: 4.2023677230 \t valid acc: 0.3336\n",
      "Epoch 3375 \t train loss: 4.1062257678 \t valid loss: 4.2176265717 \t valid acc: 0.3359\n",
      "Epoch 3380 \t train loss: 4.1339555674 \t valid loss: 4.1992284060 \t valid acc: 0.3352\n",
      "Epoch 3385 \t train loss: 4.1297962167 \t valid loss: 4.2050413489 \t valid acc: 0.3344\n",
      "Epoch 3390 \t train loss: 4.0787007809 \t valid loss: 4.2104193568 \t valid acc: 0.3352\n",
      "Epoch 3395 \t train loss: 4.0697441822 \t valid loss: 4.2059311271 \t valid acc: 0.3359\n",
      "Epoch 3400 \t train loss: 4.1266228210 \t valid loss: 4.2008808255 \t valid acc: 0.3281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3405 \t train loss: 4.0809549010 \t valid loss: 4.2219447494 \t valid acc: 0.3313\n",
      "Epoch 3410 \t train loss: 4.1086711662 \t valid loss: 4.2113816738 \t valid acc: 0.3344\n",
      "Epoch 3415 \t train loss: 4.1664775416 \t valid loss: 4.2005615234 \t valid acc: 0.3375\n",
      "Epoch 3420 \t train loss: 4.1166983316 \t valid loss: 4.2160872221 \t valid acc: 0.3352\n",
      "Epoch 3425 \t train loss: 4.0653869607 \t valid loss: 4.2189937234 \t valid acc: 0.3375\n",
      "Epoch 3430 \t train loss: 4.1194869418 \t valid loss: 4.2038389444 \t valid acc: 0.3359\n",
      "Epoch 3435 \t train loss: 4.1097669990 \t valid loss: 4.1950494647 \t valid acc: 0.3367\n",
      "Epoch 3440 \t train loss: 4.0850865120 \t valid loss: 4.1982666850 \t valid acc: 0.3352\n",
      "Epoch 3445 \t train loss: 4.0994258260 \t valid loss: 4.2104794383 \t valid acc: 0.3352\n",
      "Epoch 3450 \t train loss: 4.0952711660 \t valid loss: 4.2067479491 \t valid acc: 0.3336\n",
      "Epoch 3455 \t train loss: 4.1314848190 \t valid loss: 4.2088178992 \t valid acc: 0.3328\n",
      "Epoch 3460 \t train loss: 4.1016249490 \t valid loss: 4.2070726752 \t valid acc: 0.3305\n",
      "Epoch 3465 \t train loss: 4.0962391288 \t valid loss: 4.2123365402 \t valid acc: 0.3313\n",
      "Epoch 3470 \t train loss: 4.1334027634 \t valid loss: 4.2258741260 \t valid acc: 0.3305\n",
      "Epoch 3475 \t train loss: 4.0965304042 \t valid loss: 4.2144389749 \t valid acc: 0.3313\n",
      "Epoch 3480 \t train loss: 4.1185460146 \t valid loss: 4.2078930736 \t valid acc: 0.3289\n",
      "Epoch 3485 \t train loss: 4.0927270013 \t valid loss: 4.2202198505 \t valid acc: 0.3336\n",
      "Epoch 3490 \t train loss: 4.0824752797 \t valid loss: 4.2204338908 \t valid acc: 0.3281\n",
      "Epoch 3495 \t train loss: 4.1197862459 \t valid loss: 4.2016523480 \t valid acc: 0.3352\n",
      "Epoch 3500 \t train loss: 4.0911752790 \t valid loss: 4.1947033405 \t valid acc: 0.3305\n",
      "Epoch 3505 \t train loss: 4.0547183004 \t valid loss: 4.1944081187 \t valid acc: 0.3305\n",
      "Epoch 3510 \t train loss: 4.1351717295 \t valid loss: 4.2110915184 \t valid acc: 0.3344\n",
      "Epoch 3515 \t train loss: 4.0824075078 \t valid loss: 4.2076032758 \t valid acc: 0.3305\n",
      "Epoch 3520 \t train loss: 4.1001840192 \t valid loss: 4.2014099360 \t valid acc: 0.3328\n",
      "Epoch 3525 \t train loss: 4.0627030716 \t valid loss: 4.1911505461 \t valid acc: 0.3320\n",
      "Epoch 3530 \t train loss: 4.1073715021 \t valid loss: 4.2194795609 \t valid acc: 0.3305\n",
      "Epoch 3535 \t train loss: 4.0981869199 \t valid loss: 4.2137219906 \t valid acc: 0.3297\n",
      "Epoch 3540 \t train loss: 4.0600813211 \t valid loss: 4.2119650245 \t valid acc: 0.3281\n",
      "Epoch 3545 \t train loss: 4.0764033018 \t valid loss: 4.1970568895 \t valid acc: 0.3344\n",
      "Epoch 3550 \t train loss: 4.1321559673 \t valid loss: 4.1979262233 \t valid acc: 0.3328\n",
      "Epoch 3555 \t train loss: 4.0548910817 \t valid loss: 4.2098883390 \t valid acc: 0.3273\n",
      "Epoch 3560 \t train loss: 4.0908898586 \t valid loss: 4.2073748708 \t valid acc: 0.3305\n",
      "Epoch 3565 \t train loss: 4.1190479467 \t valid loss: 4.2155460715 \t valid acc: 0.3266\n",
      "Epoch 3570 \t train loss: 4.1043035762 \t valid loss: 4.2158829570 \t valid acc: 0.3289\n",
      "Epoch 3575 \t train loss: 4.0659452039 \t valid loss: 4.2087898254 \t valid acc: 0.3336\n",
      "Epoch 3580 \t train loss: 4.1085407401 \t valid loss: 4.1994355321 \t valid acc: 0.3289\n",
      "Epoch 3585 \t train loss: 4.1165529351 \t valid loss: 4.2135090828 \t valid acc: 0.3328\n",
      "Epoch 3590 \t train loss: 4.1002362828 \t valid loss: 4.2082729936 \t valid acc: 0.3281\n",
      "Epoch 3595 \t train loss: 4.0552575477 \t valid loss: 4.2069455385 \t valid acc: 0.3273\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "print(X_valid.shape)\n",
    "# setting hyperparameters and gettings epoch sizes\n",
    "batch_size = 32\n",
    "num_epochs = 5000\n",
    "num_samples_train = X_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = X_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "# Define a loss function and optimizer for this problem\n",
    "criterion = torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0005,momentum=0.2, weight_decay=3e-3)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    predictions = torch.round(ys)\n",
    "    correct_prediction = torch.eq(predictions,ts)\n",
    "    return torch.mean(correct_prediction.float())\n",
    "\n",
    "# Track loss\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_training_accs = 0\n",
    "    epoch_validation_loss = 0\n",
    "    epoch_validation_accs = 0\n",
    "    \n",
    "    net.eval()\n",
    "    for j in range(num_batches_valid):\n",
    "        inputs = get_variable(torch.Tensor(X_valid[j * batch_size: (j + 1) * batch_size]))\n",
    "        targets = get_variable(torch.Tensor(np.squeeze(y_valid[j * batch_size: (j + 1) * batch_size])))\n",
    "        # Forward pass\n",
    "        outputs = net.forward(inputs)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs,targets)\n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss.cpu().detach().numpy()\n",
    "        epoch_validation_accs += accuracy(outputs, targets)\n",
    "    \n",
    "    net.train()\n",
    "    for j in range(num_batches_train):\n",
    "        inputs = get_variable(torch.Tensor(X_train[j * batch_size: (j + 1) * batch_size]))\n",
    "        targets = get_variable(torch.Tensor(np.squeeze(y_train[j * batch_size: (j + 1) * batch_size])))\n",
    "        outputs = net.forward(inputs)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs,targets)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update loss\n",
    "        epoch_training_loss += loss.cpu().detach().numpy()\n",
    "        epoch_training_accs += accuracy(outputs,targets)\n",
    "        \n",
    "    # Save loss for plot\n",
    "    train_loss.append(epoch_training_loss/num_batches_train)\n",
    "    train_accs.append(epoch_training_accs/num_batches_train)\n",
    "    valid_loss.append(epoch_validation_loss/num_batches_valid)\n",
    "    valid_accs.append(epoch_validation_accs/num_batches_valid)\n",
    "\n",
    "    # Print loss every 5 epochs\n",
    "    if i % 5 == 0:\n",
    "        print(f'Epoch {i} \\t train loss: {train_loss[-1]:.10f} \\t valid loss: {valid_loss[-1]:.10f} \\t valid acc: {valid_accs[-1]:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(train_loss))\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epoch, train_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, valid_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('MSE')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch, train_accs, 'r', label='Training Accuracy')\n",
    "plt.plot(epoch, valid_accs, 'b', label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.,  8.,  5.,  4., 10.,  1.,  6.,  6.,  5.,  3.],\n",
      "        [ 0.,  2.,  1.,  2.,  6.,  2.,  0.,  6.,  1.,  0.],\n",
      "        [ 1.,  3.,  6.,  2.,  4.,  3.,  2.,  2.,  2.,  1.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  1.],\n",
      "        [ 3.,  4.,  9.,  8.,  4.,  0.,  0.,  3.,  0.,  4.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [15.,  9., 11., 14., 14., 10.,  8., 11.,  6.,  8.],\n",
      "        [ 4.,  0.,  0.,  0.,  0.,  3.,  2.,  3.,  0.,  1.],\n",
      "        [ 0.,  3.,  1.,  1.,  0.,  3.,  1.,  0.,  0.,  0.],\n",
      "        [ 1.,  1.,  0.,  0.,  3.,  0.,  0.,  0.,  1.,  0.],\n",
      "        [ 1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [10.,  2.,  0.,  2.,  2.,  2.,  0.,  1.,  0.,  2.],\n",
      "        [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 9.,  2.,  3.,  1., 10.,  1.,  2.,  8.,  3.,  7.],\n",
      "        [ 4.,  5.,  1.,  1.,  3.,  1.,  2.,  2.,  3.,  2.],\n",
      "        [ 1.,  3., 11.,  4.,  4.,  2.,  2.,  2.,  4.,  7.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 5.,  1.,  1.,  2.,  0.,  3.,  0.,  0.,  0.,  0.],\n",
      "        [ 5.,  7.,  1.,  2., 10., 12.,  0.,  8.,  2.,  4.],\n",
      "        [ 7.,  0.,  6.,  0.,  6.,  3.,  4.,  3.,  6.,  5.],\n",
      "        [ 3.,  2.,  3.,  1.,  2.,  3.,  4.,  1.,  0.,  2.],\n",
      "        [ 5.,  5.,  3.,  3.,  9.,  1.,  3.,  3.,  2.,  1.],\n",
      "        [ 5.,  9., 12.,  9.,  8., 17., 14., 20.,  4.,  4.],\n",
      "        [ 4.,  4.,  4.,  5.,  3.,  5.,  2.,  0.,  7.,  2.],\n",
      "        [ 0.,  1.,  1.,  0.,  1.,  3.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "        [ 8.,  2.,  4.,  3., 11.,  7., 11.,  6.,  3.,  9.],\n",
      "        [ 6.,  1.,  2.,  3.,  1.,  3.,  0.,  1.,  0.,  0.],\n",
      "        [ 3.,  2.,  3.,  0.,  0.,  0.,  0.,  0.,  3.,  1.],\n",
      "        [ 3.,  4., 12., 14., 16., 11.,  7., 10.,  1.,  5.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1.,  0.,  0.,  0.,  2.,  2.,  1.,  0.]], device='cuda:0')\n",
      "tensor([[ 4.8826,  5.0037,  3.8037,  3.9464,  4.8321,  3.3049,  3.5032,  3.8018,\n",
      "          3.3823,  3.2621],\n",
      "        [ 2.9292,  3.2674,  3.6817,  3.9639,  3.1637,  3.4567,  2.9813,  2.7407,\n",
      "          2.2122,  2.3286],\n",
      "        [ 1.2275,  1.4956,  1.6965,  1.3696,  1.0922,  1.6101,  0.8286,  0.9001,\n",
      "          0.9763,  0.7101],\n",
      "        [ 0.5989,  0.5353,  0.8594,  0.7670,  0.3793,  0.6835,  0.5229,  0.3847,\n",
      "          0.3765,  0.1821],\n",
      "        [ 4.7029,  4.7197,  5.5725,  5.5647,  4.7838,  4.7722,  4.7097,  3.9958,\n",
      "          3.0093,  3.4917],\n",
      "        [ 1.5299,  1.6031,  1.0827,  0.9398,  1.1382,  0.9275,  0.7994,  0.8561,\n",
      "          0.9950,  0.7280],\n",
      "        [ 7.2077,  6.8415,  5.4114,  5.8617,  6.7658,  4.6713,  5.0304,  5.2986,\n",
      "          4.7385,  4.6675],\n",
      "        [ 1.1114,  1.1936,  1.3389,  0.9920,  0.7948,  1.2150,  0.8284,  0.7477,\n",
      "          0.7854,  0.5182],\n",
      "        [ 0.4776,  0.7139,  1.0976,  0.7068,  0.0782,  1.3280,  0.3460,  0.1276,\n",
      "          0.6359, -0.0553],\n",
      "        [ 1.5226,  1.2820,  1.2987,  1.0765,  1.3615,  1.0921,  0.7322,  1.0252,\n",
      "          0.8629,  0.8099],\n",
      "        [ 0.5816,  0.3587,  0.3744,  0.0950,  0.4390,  0.2515,  0.2132,  0.3193,\n",
      "          0.2941,  0.2368],\n",
      "        [ 2.4666,  2.1234,  1.7020,  1.7314,  2.0894,  1.2485,  1.5516,  1.6067,\n",
      "          1.5461,  1.5188],\n",
      "        [ 1.2865,  0.6693,  0.6706,  0.4100,  0.9165,  0.3120,  0.6179,  0.4609,\n",
      "          0.5079,  0.6603],\n",
      "        [ 7.2919,  6.5286,  4.9445,  5.2296,  6.7156,  4.0127,  4.8370,  4.9759,\n",
      "          4.6284,  4.6721],\n",
      "        [ 3.5162,  3.3184,  2.4649,  2.6074,  3.1081,  1.9564,  2.2397,  2.4060,\n",
      "          2.2435,  2.2495],\n",
      "        [ 5.5534,  5.5921,  4.1071,  4.4291,  5.0212,  3.5230,  3.7043,  4.1363,\n",
      "          3.6706,  3.6687],\n",
      "        [ 0.4838,  0.3066,  0.0108, -0.1216,  0.0516, -0.0548, -0.1833,  0.0288,\n",
      "          0.4460,  0.0253],\n",
      "        [ 2.1938,  2.0866,  1.9946,  2.2275,  1.4635,  2.0492,  1.4532,  1.4312,\n",
      "          1.5138,  1.1091],\n",
      "        [ 7.6269,  7.1346,  5.4432,  6.0121,  6.7694,  4.7629,  5.0854,  5.4735,\n",
      "          4.8437,  4.7790],\n",
      "        [ 3.6271,  3.1053,  2.6192,  2.9028,  3.1222,  2.2080,  2.4241,  2.4940,\n",
      "          2.1790,  2.2382],\n",
      "        [ 1.9536,  1.8659,  2.2559,  2.1285,  1.6726,  1.8231,  1.7752,  1.4308,\n",
      "          1.2685,  1.4838],\n",
      "        [ 4.1062,  3.6993,  4.2667,  4.2804,  3.9920,  3.6721,  3.5764,  3.2080,\n",
      "          2.4986,  2.7809],\n",
      "        [ 6.6386,  6.6497,  5.4144,  5.8940,  6.2077,  4.8773,  4.7447,  4.9380,\n",
      "          4.6504,  4.4503],\n",
      "        [ 4.5696,  4.5357,  3.6232,  3.8917,  4.0933,  3.2204,  3.1126,  3.4754,\n",
      "          3.2333,  2.9768],\n",
      "        [ 0.4658,  0.5293,  0.5957,  0.4955,  0.3325,  0.6343,  0.2652,  0.2103,\n",
      "          0.4502,  0.0402],\n",
      "        [ 0.6645,  0.9594,  1.1589,  0.7516,  0.7016,  1.1640,  0.5550,  0.6910,\n",
      "          0.6840,  0.4417],\n",
      "        [ 5.1388,  5.0890,  6.3926,  6.2169,  5.1914,  5.5745,  5.1909,  4.3970,\n",
      "          3.1124,  3.9388],\n",
      "        [ 2.8987,  2.7930,  3.7451,  3.7611,  2.7671,  3.3296,  2.8832,  2.2615,\n",
      "          1.6751,  2.2435],\n",
      "        [ 1.4905,  1.8045,  1.6259,  1.3090,  1.3754,  1.6988,  1.0532,  1.1047,\n",
      "          1.0766,  0.9175],\n",
      "        [ 5.9977,  5.6752,  6.1638,  6.3153,  5.6493,  5.3426,  5.2450,  4.6078,\n",
      "          3.8191,  4.1089],\n",
      "        [ 0.8427,  0.3657,  0.5303,  0.6147,  0.4197,  0.4240,  0.3412,  0.3691,\n",
      "          0.2932,  0.3228],\n",
      "        [ 0.0776,  0.1083,  1.0444,  0.5855, -0.1647,  0.9118,  0.3706,  0.0723,\n",
      "          0.2276,  0.0260]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[5., 5., 4., 4., 5., 3., 4., 4., 3., 3.],\n",
      "        [3., 3., 4., 4., 3., 3., 3., 3., 2., 2.],\n",
      "        [1., 1., 2., 1., 1., 2., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 0., 1., 1., 0., 0., 0.],\n",
      "        [5., 5., 6., 6., 5., 5., 5., 4., 3., 3.],\n",
      "        [2., 2., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [7., 7., 5., 6., 7., 5., 5., 5., 5., 5.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 0., 1., 0., 0., 1., -0.],\n",
      "        [2., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [2., 2., 2., 2., 2., 1., 2., 2., 2., 2.],\n",
      "        [1., 1., 1., 0., 1., 0., 1., 0., 1., 1.],\n",
      "        [7., 7., 5., 5., 7., 4., 5., 5., 5., 5.],\n",
      "        [4., 3., 2., 3., 3., 2., 2., 2., 2., 2.],\n",
      "        [6., 6., 4., 4., 5., 4., 4., 4., 4., 4.],\n",
      "        [0., 0., 0., -0., 0., -0., -0., 0., 0., 0.],\n",
      "        [2., 2., 2., 2., 1., 2., 1., 1., 2., 1.],\n",
      "        [8., 7., 5., 6., 7., 5., 5., 5., 5., 5.],\n",
      "        [4., 3., 3., 3., 3., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 1., 1., 1.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 3., 2., 3.],\n",
      "        [7., 7., 5., 6., 6., 5., 5., 5., 5., 4.],\n",
      "        [5., 5., 4., 4., 4., 3., 3., 3., 3., 3.],\n",
      "        [0., 1., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [5., 5., 6., 6., 5., 6., 5., 4., 3., 4.],\n",
      "        [3., 3., 4., 4., 3., 3., 3., 2., 2., 2.],\n",
      "        [1., 2., 2., 1., 1., 2., 1., 1., 1., 1.],\n",
      "        [6., 6., 6., 6., 6., 5., 5., 5., 4., 4.],\n",
      "        [1., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., -0., 1., 0., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<RoundBackward>)\n",
      "tensor([[ 2.8567e-01,  3.7495e-01,  1.9996e-01,  0.0000e+00,  4.9995e-01,\n",
      "         -1.9980e+00,  3.3328e-01,  3.3328e-01,  3.9992e-01,  0.0000e+00],\n",
      "        [-3.0000e+03, -4.9975e-01, -2.9970e+00, -9.9950e-01,  4.9992e-01,\n",
      "         -4.9975e-01, -3.0000e+03,  4.9992e-01, -9.9900e-01, -2.0000e+03],\n",
      "        [ 0.0000e+00,  6.6644e-01,  6.6656e-01,  4.9975e-01,  7.4981e-01,\n",
      "          3.3322e-01,  4.9975e-01,  4.9975e-01,  4.9975e-01,  0.0000e+00],\n",
      "        [-1.0000e+03,  0.0000e+00, -1.0000e+03, -1.0000e+03,  0.0000e+00,\n",
      "         -1.0000e+03,  0.0000e+00,  0.0000e+00,  9.9900e-01,  9.9900e-01],\n",
      "        [-6.6644e-01, -2.4994e-01,  3.3330e-01,  2.4997e-01, -2.4994e-01,\n",
      "         -5.0000e+03, -5.0000e+03, -3.3322e-01, -3.0000e+03,  2.4994e-01],\n",
      "        [-9.9900e-01, -2.0000e+03, -1.0000e+03, -1.0000e+03, -1.0000e+03,\n",
      "         -1.0000e+03, -1.0000e+03, -1.0000e+03, -1.0000e+03, -1.0000e+03],\n",
      "        [ 5.3330e-01,  2.2220e-01,  5.4540e-01,  5.7139e-01,  4.9996e-01,\n",
      "          4.9995e-01,  3.7495e-01,  5.4540e-01,  1.6664e-01,  3.7495e-01],\n",
      "        [ 7.4981e-01, -1.0000e+03, -1.0000e+03, -1.0000e+03, -1.0000e+03,\n",
      "          6.6644e-01,  4.9975e-01,  6.6644e-01, -1.0000e+03,  0.0000e+00],\n",
      "        [ 0.0000e+00,  6.6644e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          6.6644e-01,  9.9900e-01,  0.0000e+00, -1.0000e+03,  0.0000e+00],\n",
      "        [-9.9900e-01,  0.0000e+00, -1.0000e+03, -1.0000e+03,  6.6644e-01,\n",
      "         -1.0000e+03, -1.0000e+03, -1.0000e+03,  0.0000e+00, -1.0000e+03],\n",
      "        [ 0.0000e+00,  0.0000e+00,  9.9900e-01,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 7.9992e-01,  0.0000e+00, -2.0000e+03,  0.0000e+00,  0.0000e+00,\n",
      "          4.9975e-01, -2.0000e+03, -9.9900e-01, -2.0000e+03,  0.0000e+00],\n",
      "        [ 0.0000e+00, -1.0000e+03, -1.0000e+03,  0.0000e+00, -1.0000e+03,\n",
      "          0.0000e+00, -1.0000e+03,  0.0000e+00, -1.0000e+03, -1.0000e+03],\n",
      "        [ 2.2220e-01, -2.4988e+00, -6.6644e-01, -3.9960e+00,  2.9997e-01,\n",
      "         -2.9970e+00, -1.4993e+00,  3.7495e-01, -6.6644e-01,  2.8567e-01],\n",
      "        [ 0.0000e+00,  3.9992e-01, -9.9900e-01, -1.9980e+00,  0.0000e+00,\n",
      "         -9.9900e-01,  0.0000e+00,  0.0000e+00,  3.3322e-01,  0.0000e+00],\n",
      "        [-4.9950e+00, -9.9967e-01,  6.3631e-01,  0.0000e+00, -2.4994e-01,\n",
      "         -9.9950e-01, -9.9950e-01, -9.9950e-01,  0.0000e+00,  4.2851e-01],\n",
      "        [ 0.0000e+00,  9.9900e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 5.9988e-01, -9.9900e-01, -9.9900e-01,  0.0000e+00, -1.0000e+03,\n",
      "          3.3322e-01, -1.0000e+03, -1.0000e+03, -2.0000e+03, -1.0000e+03],\n",
      "        [-5.9988e-01,  0.0000e+00, -3.9960e+00, -1.9990e+00,  2.9997e-01,\n",
      "          5.8328e-01, -5.0000e+03,  3.7495e-01, -1.4993e+00, -2.4994e-01],\n",
      "        [ 4.2851e-01, -3.0000e+03,  4.9992e-01, -3.0000e+03,  4.9992e-01,\n",
      "          3.3322e-01,  4.9988e-01,  3.3322e-01,  6.6656e-01,  5.9988e-01],\n",
      "        [ 3.3322e-01,  0.0000e+00,  3.3322e-01, -9.9900e-01,  0.0000e+00,\n",
      "          3.3322e-01,  4.9988e-01,  0.0000e+00, -1.0000e+03,  4.9975e-01],\n",
      "        [ 1.9996e-01,  1.9996e-01, -3.3322e-01, -3.3322e-01,  5.5549e-01,\n",
      "         -2.9970e+00, -3.3322e-01,  0.0000e+00,  0.0000e+00, -1.9980e+00],\n",
      "        [-3.9992e-01,  2.2220e-01,  5.8328e-01,  3.3330e-01,  2.4997e-01,\n",
      "          7.0584e-01,  6.4281e-01,  7.4996e-01, -2.4994e-01,  0.0000e+00],\n",
      "        [-2.4994e-01, -2.4994e-01,  0.0000e+00,  1.9996e-01, -3.3322e-01,\n",
      "          3.9992e-01, -4.9975e-01, -3.0000e+03,  5.7135e-01, -4.9975e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  9.9900e-01,\n",
      "          6.6644e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.0000e+03, -1.0000e+03, -1.0000e+03, -1.0000e+03, -1.0000e+03,\n",
      "          0.0000e+00, -1.0000e+03, -1.0000e+03, -1.0000e+03,  0.0000e+00],\n",
      "        [ 3.7495e-01, -1.4993e+00, -4.9988e-01, -9.9967e-01,  5.4540e-01,\n",
      "          1.4284e-01,  5.4540e-01,  3.3328e-01,  0.0000e+00,  5.5549e-01],\n",
      "        [ 4.9992e-01, -1.9980e+00, -9.9950e-01, -3.3322e-01, -1.9980e+00,\n",
      "          0.0000e+00, -3.0000e+03, -9.9900e-01, -2.0000e+03, -2.0000e+03],\n",
      "        [ 6.6644e-01,  0.0000e+00,  3.3322e-01, -1.0000e+03, -1.0000e+03,\n",
      "         -2.0000e+03, -1.0000e+03, -1.0000e+03,  6.6644e-01,  0.0000e+00],\n",
      "        [-9.9967e-01, -4.9988e-01,  4.9996e-01,  5.7139e-01,  6.2496e-01,\n",
      "          5.4540e-01,  2.8567e-01,  4.9995e-01, -2.9970e+00,  1.9996e-01],\n",
      "        [-1.0000e+03,  0.0000e+00, -1.0000e+03, -1.0000e+03,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 9.9900e-01,  9.9900e-01,  0.0000e+00, -1.0000e+03,  0.0000e+00,\n",
      "         -1.0000e+03,  9.9950e-01,  9.9950e-01,  9.9900e-01,  0.0000e+00]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(targets)\n",
    "print(outputs)\n",
    "print(torch.round(outputs))\n",
    "print(torch.div(targets-torch.round(outputs),targets+0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-torch-gpu]",
   "language": "python",
   "name": "conda-env-ml-torch-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
